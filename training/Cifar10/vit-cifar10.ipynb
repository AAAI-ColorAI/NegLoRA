{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2491748,"sourceType":"datasetVersion","datasetId":1500837}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"### Necessary Imports and dependencies\n### Wandb project_name is baseline_ImageNet\nimport os\nimport shutil\nimport time\nimport math\nfrom enum import Enum\nfrom functools import partial\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as datasets\nfrom torchvision.transforms import v2\nimport torchvision.transforms as transforms\nfrom typing import Any, Dict, Union, Type, Callable, Optional, List\nfrom torchvision.models.vision_transformer import MLPBlock\nimport wandb\n\n\nnum_epochs=90\n\n# Parameters specific to CIFAR-10\nbatch_size = 128\nnum_workers = 4 \n\n# Dataset loading code\n# Define CIFAR-10 datasets\ntrain_dataset = datasets.CIFAR10(\n    root='./data',\n    train=True,\n    download=True,\n    transform=transforms.Compose([\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n    ])\n)\n\nval_dataset = datasets.CIFAR10(\n    root='./data',\n    train=False,\n    download=True,\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261]),\n    ])\n)\n\nn = len(train_dataset)\n\ntotal_steps = round((n * num_epochs) / batch_size)\n\nstart_step=0\n\nmixup = v2.MixUp(alpha=0.2, num_classes=10)\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=num_workers,\n    pin_memory=True,\n    collate_fn=lambda batch: mixup(*torch.utils.data.default_collate(batch)), \n    drop_last=True\n)\n\nval_loader = torch.utils.data.DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True\n)\n\nwarmup_try=10000\n\n# Taken from https://github.com/lucidrains/vit-pytorch, likely ported from https://github.com/google-research/big_vision/\ndef posemb_sincos_2d(h, w, dim, temperature: int = 10000, dtype = torch.float32):\n    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n    assert (dim % 4) == 0, \"feature dimension must be multiple of 4 for sincos emb\"\n    omega = torch.arange(dim // 4) / (dim // 4 - 1)\n    omega = 1.0 / (temperature ** omega)\n\n    y = y.flatten()[:, None] * omega[None, :]\n    x = x.flatten()[:, None] * omega[None, :]\n    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)\n    return pe.type(dtype)\n\n\nclass EncoderBlock(nn.Module):\n    \"\"\"Transformer encoder block.\"\"\"\n\n    def __init__(\n        self,\n        num_heads: int,\n        hidden_dim: int,\n        mlp_dim: int,\n        dropout: float,\n        attention_dropout: float,\n        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n\n        # Attention block\n        self.ln_1 = norm_layer(hidden_dim)\n        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n\n        # MLP block\n        self.ln_2 = norm_layer(hidden_dim)\n        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n\n        # Fix init discrepancy between nn.MultiheadAttention and that of big_vision\n        bound = math.sqrt(3 / hidden_dim)\n        nn.init.uniform_(self.self_attention.in_proj_weight, -bound, bound)\n        nn.init.uniform_(self.self_attention.out_proj.weight, -bound, bound)\n\n    def forward(self, input: torch.Tensor):\n        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n        x = self.ln_1(input)\n        x, _ = self.self_attention(x, x, x, need_weights=False)\n        x = self.dropout(x)\n        x = x + input\n\n        y = self.ln_2(x)\n        y = self.mlp(y)\n        return x + y\n\n\nclass Encoder(nn.Module):\n    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n\n    def __init__(\n        self,\n        seq_length: int,\n        num_layers: int,\n        num_heads: int,\n        hidden_dim: int,\n        mlp_dim: int,\n        dropout: float,\n        attention_dropout: float,\n        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n    ):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        layers: OrderedDict[str, nn.Module] = OrderedDict()\n        for i in range(num_layers):\n            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n                num_heads,\n                hidden_dim,\n                mlp_dim,\n                dropout,\n                attention_dropout,\n                norm_layer,\n            )\n        self.layers = nn.Sequential(layers)\n        self.ln = norm_layer(hidden_dim)\n\n    def forward(self, input: torch.Tensor):\n        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n        return self.ln(self.layers(self.dropout(input)))\n\n\nclass SimpleVisionTransformer(nn.Module):\n    \"\"\"Vision Transformer modified per https://arxiv.org/abs/2205.01580.\"\"\"\n\n    def __init__(\n        self,\n        image_size: int,\n        patch_size: int,\n        num_layers: int,\n        num_heads: int,\n        hidden_dim: int,\n        mlp_dim: int,\n        dropout: float = 0.0,\n        attention_dropout: float = 0.0,\n        num_classes: int = 10,\n        representation_size: Optional[int] = None,\n        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n    ):\n        super().__init__()\n        torch._assert(image_size % patch_size == 0, \"Input shape indivisible by patch size!\")\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.hidden_dim = hidden_dim\n        self.mlp_dim = mlp_dim\n        self.attention_dropout = attention_dropout\n        self.dropout = dropout\n        self.num_classes = num_classes\n        self.representation_size = representation_size\n        self.norm_layer = norm_layer\n\n        self.conv_proj = nn.Conv2d(\n            in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n        h = w = image_size // patch_size\n        seq_length = h * w\n        self.register_buffer(\"pos_embedding\", posemb_sincos_2d(h=h, w=w, dim=hidden_dim))\n\n        self.encoder = Encoder(\n            seq_length,\n            num_layers,\n            num_heads,\n            hidden_dim,\n            mlp_dim,\n            dropout,\n            attention_dropout,\n            norm_layer,\n        )\n        self.seq_length = seq_length\n\n        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n        if representation_size is None:\n            heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n        else:\n            heads_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n            heads_layers[\"act\"] = nn.Tanh()\n            heads_layers[\"head\"] = nn.Linear(representation_size, num_classes)\n\n        self.heads = nn.Sequential(heads_layers)\n\n        if isinstance(self.conv_proj, nn.Conv2d):\n            # Init the patchify stem\n            fan_in = self.conv_proj.in_channels * self.conv_proj.kernel_size[0] * self.conv_proj.kernel_size[1]\n            # constant is stddev of standard normal truncated to (-2, 2)\n            std = math.sqrt(1 / fan_in) / .87962566103423978\n            nn.init.trunc_normal_(self.conv_proj.weight, std=std, a=-2 * std, b=2 * std)\n            if self.conv_proj.bias is not None:\n                nn.init.zeros_(self.conv_proj.bias)\n        elif self.conv_proj.conv_last is not None and isinstance(self.conv_proj.conv_last, nn.Conv2d):\n            # Init the last 1x1 conv of the conv stem\n            nn.init.normal_(\n                self.conv_proj.conv_last.weight, mean=0.0, std=math.sqrt(2.0 / self.conv_proj.conv_last.out_channels)\n            )\n            if self.conv_proj.conv_last.bias is not None:\n                nn.init.zeros_(self.conv_proj.conv_last.bias)\n\n        if hasattr(self.heads, \"pre_logits\") and isinstance(self.heads.pre_logits, nn.Linear):\n            fan_in = self.heads.pre_logits.in_features\n            nn.init.trunc_normal_(self.heads.pre_logits.weight, std=math.sqrt(1 / fan_in))\n            nn.init.zeros_(self.heads.pre_logits.bias)\n\n        if isinstance(self.heads.head, nn.Linear):\n            nn.init.zeros_(self.heads.head.weight)\n            nn.init.zeros_(self.heads.head.bias)\n\n    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n        n, c, h, w = x.shape\n        p = self.patch_size\n        torch._assert(h == self.image_size, f\"Wrong image height! Expected {self.image_size} but got {h}!\")\n        torch._assert(w == self.image_size, f\"Wrong image width! Expected {self.image_size} but got {w}!\")\n        n_h = h // p\n        n_w = w // p\n\n        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n        x = self.conv_proj(x)\n        \n        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n        x = x.reshape(n, self.hidden_dim, n_h * n_w)\n        \n        \n        \n        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n        # The self attention layer expects inputs in the format (N, S, E)\n        # where S is the source sequence length, N is the batch size, E is the\n        # embedding dimension\n        x = x.permute(0, 2, 1)\n        \n        return x\n\n    def forward(self, x: torch.Tensor):\n        # Reshape and permute the input tensor\n        x = self._process_input(x)\n        x = x + self.pos_embedding\n        x = self.encoder(x)\n        x = x.mean(dim = 1)\n        x = self.heads(x)\n\n        return x\n    \ndef weight_decay_param(n, p):\n    return p.ndim >= 2 and n.endswith('weight')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# create model\nmodel = SimpleVisionTransformer(\n    image_size=32,\n    patch_size=4,\n    num_layers=12,\n    num_heads=6,\n    hidden_dim=384,\n    mlp_dim=1536,\n).to(device)\nwd_params = [p for n, p in model.named_parameters() if weight_decay_param(n, p) and p.requires_grad]\nnon_wd_params = [p for n, p in model.named_parameters() if not weight_decay_param(n, p) and p.requires_grad]\n\noriginal_model = model\n\nweight_decay = 0.1\nlearning_rate = 1e-3\n\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.AdamW(\n    [\n        {\"params\": wd_params, \"weight_decay\": 0.1},\n        {\"params\": non_wd_params, \"weight_decay\": 0.},\n    ],\n    lr=learning_rate,\n)\n\nwarmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: step / warmup_try)\ncosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps - warmup_try)\nscheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, [warmup, cosine], [warmup_try])\n\n## Path_to_be_changed\ncheckpoint_path = \"/kaggle/working/\"\n\ndef save_checkpoint(state, is_best, path, filename='baselinecheckpoint_imagenet.pth.tar'):\n    filename = os.path.join(path, filename)\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, os.path.join(path, 'model_best.pth.tar'))\n\nclass Summary(Enum):\n    NONE = 0\n    AVERAGE = 1\n    SUM = 2\n    COUNT = 3\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f', summary_type=Summary.AVERAGE):\n        self.name = name\n        self.fmt = fmt\n        self.summary_type = summary_type\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def all_reduce(self):\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda\")\n        elif torch.backends.mps.is_available():\n            device = torch.device(\"mps\")\n        else:\n            device = torch.device(\"cpu\")\n        total = torch.tensor([self.sum, self.count], dtype=torch.float32, device=device)\n        dist.all_reduce(total, dist.ReduceOp.SUM, async_op=False)\n        self.sum, self.count = total.tolist()\n        self.avg = self.sum / self.count\n    \n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n    \n    def summary(self):\n        fmtstr = ''\n        if self.summary_type is Summary.NONE:\n            fmtstr = ''\n        elif self.summary_type is Summary.AVERAGE:\n            fmtstr = '{name} {avg:.3f}'\n        elif self.summary_type is Summary.SUM:\n            fmtstr = '{name} {sum:.3f}'\n        elif self.summary_type is Summary.COUNT:\n            fmtstr = '{name} {count:.3f}'\n        else:\n            raise ValueError('invalid summary type %r' % self.summary_type)\n        \n        return fmtstr.format(**self.__dict__)\n    \nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n        \n    def display_summary(self):\n        entries = [\" *\"]\n        entries += [meter.summary() for meter in self.meters]\n        print(' '.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n    \ndef accuracy(output, target, topk=(1,), class_prob=False):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n        \n        # with e.g. MixUp target is now given by probabilities for each class so we need to convert to class indices\n        if class_prob:\n            _, target = target.topk(1, 1, True, True)\n            target = target.squeeze(dim=1)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(1.0 / batch_size))\n        return res\n    \nlog_steps = 2500\n\nwandb.login(key=\"2d1b2da6b789a71e0c259cede4c9b770b2e44281\")\n\n# Initialize a new run\nwandb.init(project=\"fractual_transformer\", name=\"Baseline_ImageNet_run\")\n\ndef validate(val_loader, model, criterion, step, use_wandb=False, accum_freq=1, print_freq=100):\n    \n    def run_validate(loader, base_progress=0):\n        with torch.no_grad():\n            torch.cuda.empty_cache()\n            end = time.time()\n            for i, (images, target) in enumerate(loader):\n                i = base_progress + i\n\n                if torch.cuda.is_available():\n                    images = images.cuda(non_blocking=True)\n                    target = target.cuda(non_blocking=True)\n                elif torch.backends.mps.is_available():\n                    images = images.to('mps')\n                    target = target.to('mps')\n\n                for img, trt in zip(images.chunk(accum_freq), target.chunk(accum_freq)):\n                    # compute output\n                    output = model(img)\n                    loss = criterion(output, trt)\n\n                    # measure accuracy and record loss\n                    acc1, acc5 = accuracy(output, trt, topk=(1, 5))\n                    losses.update(loss.item(), img.size(0))\n                    top1.update(acc1[0].item(), img.size(0))\n                    top5.update(acc5[0].item(), img.size(0))\n                    \n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                if i % print_freq == 0:\n                    progress.display(i)\n\n    batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n    losses = AverageMeter('Loss', ':.4e', Summary.NONE)\n    top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n    top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n    progress = ProgressMeter(\n        len(val_loader),\n        [batch_time, losses, top1, top5],\n        prefix='Test: ')\n\n    # switch to evaluate mode\n    model.eval()\n\n    run_validate(val_loader)\n\n    progress.display_summary()\n\n    if use_wandb:        \n        log_data = {\n            'val/loss': losses.avg,\n            'val/acc@1': top1.avg,\n            'val/acc@5': top5.avg,\n        }\n        wandb.log(log_data, step=step)\n\n    return top1.avg\n\ndef train(train_loader, val_loader, start_step, total_steps, original_model, model, criterion, optimizer, scheduler, device):\n    batch_time = AverageMeter('Time', ':6.3f')\n    data_time = AverageMeter('Data', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    print_freq = 100  # Print frequency (adjust as needed)\n    log_steps = 2500  # Log steps (adjust as needed)\n    accum_freq = 1  # Gradient accumulation frequency (adjust as needed)\n    \n    progress = ProgressMeter(\n        total_steps,\n        [batch_time, data_time, losses, top1, top5]\n    )\n\n    # switch to train mode\n    model.train()\n    end = time.time()\n    best_acc1 = 0\n\n    def infinite_loader():\n        while True:\n            yield from train_loader\n\n    for step, (images, target) in zip(range(start_step + 1, total_steps + 1), infinite_loader()):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        # move data to the same device as model\n        images = images.to(device, non_blocking=True)\n        target = target.to(device, non_blocking=True)\n        step_loss = step_acc1 = step_acc5 = 0.0\n\n        for img, trt in zip(images.chunk(accum_freq), target.chunk(accum_freq)):\n            # compute output\n            output = model(img)\n            loss = criterion(output, trt)\n\n            # measure accuracy and record loss\n            acc1, acc5 = accuracy(output, trt, topk=(1, 5), class_prob=True)\n            step_loss += loss.item()\n            step_acc1 += acc1[0].item()\n            step_acc5 += acc5[0].item()\n            \n            # compute gradient\n            (loss / accum_freq).backward()\n\n        step_loss /= accum_freq\n        step_acc1 /= accum_freq\n        step_acc5 /= accum_freq\n\n        losses.update(step_loss, images.size(0))\n        top1.update(step_acc1, images.size(0))\n        top5.update(step_acc5, images.size(0))\n\n        # do SGD step\n        l2_grads = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        if step % print_freq == 0:\n            print(step)\n            progress.display(step)\n            if wandb:\n                \n                with torch.no_grad():\n                    l2_params = sum(p.square().sum().item() for _, p in model.named_parameters())\n\n                samples_per_second_per_gpu = batch_size / batch_time.val\n                samples_per_second = samples_per_second_per_gpu \n                log_data = {\n                    \"train/loss\": step_loss,\n                    'train/acc@1': step_acc1,\n                    'train/acc@5': step_acc5,\n                    \"data_time\": data_time.val,\n                    \"batch_time\": batch_time.val,\n                    \"samples_per_second\": samples_per_second,\n                    \"samples_per_second_per_gpu\": samples_per_second_per_gpu,\n                    \"lr\": scheduler.get_last_lr()[0],\n                    \"l2_grads\": l2_grads.item(),\n                    \"l2_params\": math.sqrt(l2_params)\n                }\n                wandb.log(log_data, step=step)\n\n        if step % log_steps == 0 or step == total_steps:\n\n            acc1 = validate(val_loader, original_model, criterion, step)\n\n            # remember best acc@1 and save checkpoint\n            is_best = acc1 > best_acc1\n            best_acc1 = max(acc1, best_acc1)\n            \n            save_checkpoint({\n                'step': step,\n                'state_dict': original_model.state_dict(),\n                'best_acc1': best_acc1,\n                'optimizer' : optimizer.state_dict(),\n                'scheduler' : scheduler.state_dict()\n            }, is_best,checkpoint_path)\n\n        scheduler.step()\n        \ntrain(train_loader, val_loader, start_step, total_steps, original_model, model, criterion, optimizer, scheduler, device)\n\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-09-05T10:41:43.402701Z","iopub.execute_input":"2024-09-05T10:41:43.403111Z","iopub.status.idle":"2024-09-05T11:06:27.383120Z","shell.execute_reply.started":"2024-09-05T10:41:43.403073Z","shell.execute_reply":"2024-09-05T11:06:27.382042Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}