{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2491748,"sourceType":"datasetVersion","datasetId":1500837},{"sourceId":9796413,"sourceType":"datasetVersion","datasetId":6003594},{"sourceId":160373,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":136367,"modelId":159090}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install wandb","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import necessary libraries","metadata":{}},{"cell_type":"code","source":"### Necessary Imports and dependencies\nimport os\nimport shutil\nimport time\nimport math\nfrom enum import Enum\nfrom functools import partial\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data\nfrom torchvision.transforms import v2\nimport torchvision.transforms as transforms\nfrom typing import Any, Dict, Union, Type, Callable, Optional, List\nfrom torchvision.models.vision_transformer import MLPBlock\nimport wandb\nimport json\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport random\nimport torchvision.transforms.functional as TF\nimport numpy as np\nimport torchvision.transforms as transforms\nfrom PIL import ImageOps, ImageEnhance\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-11-09T16:20:46.942811Z","iopub.execute_input":"2024-11-09T16:20:46.943207Z","iopub.status.idle":"2024-11-09T16:20:52.219596Z","shell.execute_reply.started":"2024-11-09T16:20:46.943168Z","shell.execute_reply":"2024-11-09T16:20:52.218551Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2024-11-09T16:20:52.221593Z","iopub.execute_input":"2024-11-09T16:20:52.221950Z","iopub.status.idle":"2024-11-09T16:20:52.232397Z","shell.execute_reply.started":"2024-11-09T16:20:52.221912Z","shell.execute_reply":"2024-11-09T16:20:52.231040Z"},"trusted":true},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## Epochs and Batch_size","metadata":{}},{"cell_type":"code","source":"num_epochs = 90\nbatch_size = 256","metadata":{"execution":{"iopub.status.busy":"2024-11-09T16:21:57.901919Z","iopub.execute_input":"2024-11-09T16:21:57.902984Z","iopub.status.idle":"2024-11-09T16:21:57.908407Z","shell.execute_reply.started":"2024-11-09T16:21:57.902930Z","shell.execute_reply":"2024-11-09T16:21:57.907054Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Data preprocessing and Loading","metadata":{}},{"cell_type":"code","source":"class RandAugment:\n    def __init__(self, n=9, m=0.5):\n        self.n = n\n        self.m = m  # [0, 30] in paper, but we use [0, 1] for simplicity\n        self.augment_list = [\n            self.auto_contrast, self.equalize, self.rotate, self.solarize, \n            self.color, self.contrast, self.brightness, self.sharpness,\n            self.shear_x, self.shear_y, self.translate_x, self.translate_y,\n            self.posterize, self.solarize_add, self.invert, self.identity\n        ]\n\n    def __call__(self, img):\n        ops = random.choices(self.augment_list, k=self.n)\n        for op in ops:\n            img = op(img)\n        return img\n\n    def auto_contrast(self, img):\n        return ImageOps.autocontrast(img)\n\n    def equalize(self, img):\n        return ImageOps.equalize(img)\n\n    def rotate(self, img):\n        return TF.rotate(img, self.m * 30)\n\n    def solarize(self, img):\n        return TF.solarize(img, int((1 - self.m) * 255))\n\n    def color(self, img):\n        return TF.adjust_saturation(img, 1 + self.m)\n\n    def contrast(self, img):\n        return TF.adjust_contrast(img, 1 + self.m)\n\n    def brightness(self, img):\n        return TF.adjust_brightness(img, 1 + self.m)\n\n    def sharpness(self, img):\n        return ImageEnhance.Sharpness(img).enhance(1 + self.m)\n\n    def shear_x(self, img):\n        return TF.affine(img, 0, [0, 0], 1, [self.m, 0])\n\n    def shear_y(self, img):\n        return TF.affine(img, 0, [0, 0], 1, [0, self.m])\n\n    def translate_x(self, img):\n        return TF.affine(img, 0, [int(self.m * img.size[0] / 3), 0], 1, [0, 0])\n\n    def translate_y(self, img):\n        return TF.affine(img, 0, [0, int(self.m * img.size[1] / 3)], 1, [0, 0])\n\n    def posterize(self, img):\n        return TF.posterize(img, int((1 - self.m) * 8))\n\n    def solarize_add(self, img):\n        return TF.solarize(TF.adjust_brightness(img, 1 + self.m), int((1 - self.m) * 255))\n\n    def invert(self, img):\n        return TF.invert(img) if random.random() < 0.5 else img\n\n    def identity(self, img):\n        return img\n\nclass Mixup(nn.Module):\n    def __init__(self, alpha=0.8):\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, batch):\n        images, labels = batch\n        lam = np.random.beta(self.alpha, self.alpha)\n        batch_size = images.size(0)\n        index = torch.randperm(batch_size)\n        mixed_images = lam * images + (1 - lam) * images[index, :]\n        labels_a, labels_b = labels, labels[index]\n        return mixed_images, labels_a, labels_b, lam\n\nclass CutMix(nn.Module):\n    def __init__(self, alpha=1.0):\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, batch):\n        images, labels = batch\n        lam = np.random.beta(self.alpha, self.alpha)\n        batch_size, _, H, W = images.shape\n        cx = np.random.uniform(0, W)\n        cy = np.random.uniform(0, H)\n        w = W * np.sqrt(1 - lam)\n        h = H * np.sqrt(1 - lam)\n        x0 = int(np.clip(cx - w // 2, 0, W))\n        y0 = int(np.clip(cy - h // 2, 0, H))\n        x1 = int(np.clip(cx + w // 2, 0, W))\n        y1 = int(np.clip(cy + h // 2, 0, H))\n        index = torch.randperm(batch_size)\n        images[:, :, y0:y1, x0:x1] = images[index, :, y0:y1, x0:x1]\n        lam = 1 - ((x1 - x0) * (y1 - y0) / (W * H))\n        labels_a, labels_b = labels, labels[index]\n        return images, labels_a, labels_b, lam\n\nclass RandomErasing(nn.Module):\n    def __init__(self, probability=0.25, sl=0.02, sh=0.4, r1=0.3, r2=1/0.3):\n        super().__init__()\n        self.probability = probability\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n        self.r2 = r2\n\n    def forward(self, img):\n        if random.uniform(0, 1) > self.probability:\n            return img\n        \n        for attempt in range(100):\n            area = img.size()[1] * img.size()[2]\n            target_area = random.uniform(self.sl, self.sh) * area\n            aspect_ratio = random.uniform(self.r1, self.r2)\n\n            h = int(round(np.sqrt(target_area * aspect_ratio)))\n            w = int(round(np.sqrt(target_area / aspect_ratio)))\n\n            if w < img.size()[2] and h < img.size()[1]:\n                x1 = random.randint(0, img.size()[1] - h)\n                y1 = random.randint(0, img.size()[2] - w)\n                if img.size()[0] == 3:\n                    img[0, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n                    img[1, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n                    img[2, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n                else:\n                    img[0, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n                return img\n        return img\n\nclass LabelSmoothing(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super(LabelSmoothing, self).__init__()\n        self.smoothing = smoothing\n\n    def forward(self, pred, target):\n        n_classes = pred.size(1)\n        one_hot = torch.zeros_like(pred).scatter(1, target.unsqueeze(1), 1)\n        smooth_one_hot = one_hot * (1 - self.smoothing) + self.smoothing / n_classes\n        log_prob = nn.functional.log_softmax(pred, dim=1)\n        return torch.mean(torch.sum(-smooth_one_hot * log_prob, dim=1))\n\n# Updated ImageNet100Dataset\nclass ImageNet100Dataset(torch.utils.data.Dataset):\n    def __init__(self, root_dirs, labels_file, transform=None, augment=None, retain=False, forget=False, forget_label=\"n01818515\"):\n        self.transform = transform\n        self.augment = augment\n        self.images = []\n        self.labels = []\n        self.label_to_idx = {}\n        \n        with open(labels_file, 'r') as f:\n            label_dict = json.load(f)\n        \n        unique_labels = sorted(label_dict.keys())\n        self.label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n        \n        for root_dir in root_dirs:\n            for label in os.listdir(root_dir):\n                if retain:\n                    if label == forget_label:\n                        continue\n                    else:\n                        label_path = os.path.join(root_dir, label)\n                        if os.path.isdir(label_path):\n                            for img_name in os.listdir(label_path):\n                                img_path = os.path.join(label_path, img_name)\n                                self.images.append(img_path)\n                                self.labels.append(self.label_to_idx[label])\n                elif forget:\n                    if label != forget_label:\n                        continue\n                    else:\n                        label_path = os.path.join(root_dir, label)\n                        if os.path.isdir(label_path):\n                            for img_name in os.listdir(label_path):\n                                img_path = os.path.join(label_path, img_name)\n                                self.images.append(img_path)\n                                self.labels.append(self.label_to_idx[label])\n                else:\n                    label_path = os.path.join(root_dir, label)\n                    if os.path.isdir(label_path):\n                        for img_name in os.listdir(label_path):\n                            img_path = os.path.join(label_path, img_name)\n                            self.images.append(img_path)\n                            self.labels.append(self.label_to_idx[label])\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        image = Image.open(img_path).convert('RGB')\n        label = self.labels[idx]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        if self.augment:\n            image = self.augment(image)\n        \n        label = torch.tensor(label)\n        \n        return image, label\n\n# Define transformations\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.05, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    RandAugment(n=9, m=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    RandomErasing(probability=0.25)\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Create the datasets\ntrain_dirs = [\n    '/kaggle/input/imagenet100/train.X1',\n    '/kaggle/input/imagenet100/train.X2',\n    '/kaggle/input/imagenet100/train.X3',\n    '/kaggle/input/imagenet100/train.X4'\n]\nval_dir = ['/kaggle/input/imagenet100/val.X']\nlabels_file = '/kaggle/input/imagenet100/Labels.json'\n\ntrain_dataset = ImageNet100Dataset(\n    root_dirs=train_dirs,\n    labels_file=labels_file,\n    transform=train_transform,\n    retain = True\n)\n\nval_dataset = ImageNet100Dataset(\n    root_dirs=val_dir,\n    labels_file=labels_file,\n    transform=val_transform\n)\n\n# Custom collate function for Mixup and CutMix\ndef collate_fn(batch):\n    images, labels = torch.utils.data.default_collate(batch)\n    if random.random() < 0.5:\n        return Mixup(alpha=0.8)((images, labels))\n    else:\n        return CutMix(alpha=1.0)((images, labels))\n\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_fn,\n    drop_last=True\n)\n\nval_loader = torch.utils.data.DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    drop_last=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T16:21:58.634911Z","iopub.execute_input":"2024-11-09T16:21:58.635483Z","iopub.status.idle":"2024-11-09T16:22:24.572522Z","shell.execute_reply.started":"2024-11-09T16:21:58.635443Z","shell.execute_reply":"2024-11-09T16:22:24.571456Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Steps","metadata":{}},{"cell_type":"code","source":"n = len(train_dataset)\ntotal_steps = round((n * num_epochs) / batch_size)\nwarmup_try = 10000","metadata":{"execution":{"iopub.status.busy":"2024-11-09T16:22:24.574508Z","iopub.execute_input":"2024-11-09T16:22:24.575252Z","iopub.status.idle":"2024-11-09T16:22:24.580683Z","shell.execute_reply.started":"2024-11-09T16:22:24.575202Z","shell.execute_reply":"2024-11-09T16:22:24.579131Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"print(total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T16:23:54.648904Z","iopub.execute_input":"2024-11-09T16:23:54.649630Z","iopub.status.idle":"2024-11-09T16:23:54.654715Z","shell.execute_reply.started":"2024-11-09T16:23:54.649589Z","shell.execute_reply":"2024-11-09T16:23:54.653597Z"},"trusted":true},"outputs":[{"name":"stdout","text":"45703\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class MLPBlock(nn.Module):\n    def __init__(self, in_dim, mlp_dim, dropout):\n        super().__init__()\n        self.linear_1 = nn.Linear(in_dim, mlp_dim)\n        self.activation = nn.GELU()\n        self.dropout_1 = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(mlp_dim, in_dim)\n        self.dropout_2 = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.linear_1(x)\n        x = self.activation(x)\n        x = self.dropout_1(x)\n        x = self.linear_2(x)\n        x = self.dropout_2(x)\n        return x\n    \nclass EncoderBlock(nn.Module):\n    \"\"\"Transformer encoder block.\"\"\"\n\n    def __init__(\n        self,\n        num_heads: int,\n        hidden_dim: int,\n        mlp_dim: int,\n        dropout: float,\n        attention_dropout: float,\n        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n\n        # Attention block\n        self.ln_1 = norm_layer(hidden_dim)\n        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n\n        # MLP block\n        self.ln_2 = norm_layer(hidden_dim)\n        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n\n        # Fix init discrepancy between nn.MultiheadAttention and that of big_vision\n        bound = math.sqrt(3 / hidden_dim)\n        nn.init.uniform_(self.self_attention.in_proj_weight, -bound, bound)\n        nn.init.uniform_(self.self_attention.out_proj.weight, -bound, bound)\n\n    def forward(self, input: torch.Tensor):\n        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n        x = self.ln_1(input)\n        x, _ = self.self_attention(x, x, x, need_weights=False)\n        x = self.dropout(x)\n        x = x + input\n\n        y = self.ln_2(x)\n        y = self.mlp(y)\n        return x + y\n\n\nclass Encoder(nn.Module):\n    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n\n    def __init__(\n        self,\n        num_layers: int,\n        num_heads: int,\n        hidden_dim: int,\n        mlp_dim: int,\n        dropout: float,\n        attention_dropout: float,\n        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n    ):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        layers: OrderedDict[str, nn.Module] = OrderedDict()\n        for i in range(num_layers):\n            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n                num_heads,\n                hidden_dim,\n                mlp_dim,\n                dropout,\n                attention_dropout,\n                norm_layer,\n            )\n        self.layers = nn.Sequential(layers)\n        self.ln = norm_layer(hidden_dim)\n\n    def forward(self, input: torch.Tensor):\n        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n        return self.ln(self.layers(self.dropout(input)))\n\n\ndef jax_lecun_normal(layer, fan_in):\n    \"\"\"(re-)initializes layer weight in the same way as jax.nn.initializers.lecun_normal and bias to zero\"\"\"\n\n    # constant is stddev of standard normal truncated to (-2, 2)\n    std = math.sqrt(1 / fan_in) / .87962566103423978\n    nn.init.trunc_normal_(layer.weight, std=std, a=-2 * std, b=2 * std)\n    if layer.bias is not None:\n        nn.init.zeros_(layer.bias)\n        \nclass SimpleVisionTransformer(nn.Module):\n    \"\"\"Vision Transformer modified per https://arxiv.org/abs/2205.01580.\"\"\"\n    def __init__(\n        self,\n        image_size: int,\n        patch_size: int,\n        num_layers: int,\n        num_heads: int,\n        hidden_dim: int,\n        mlp_dim: int,\n        dropout: float = 0.0,\n        attention_dropout: float = 0.0,\n        num_classes: int = 100,\n        representation_size: Optional[int] = None,\n        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n    ):\n        super().__init__()\n        torch._assert(image_size % patch_size == 0, \"Input shape indivisible by patch size!\")\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.hidden_dim = hidden_dim\n        self.mlp_dim = mlp_dim\n        self.attention_dropout = attention_dropout\n        self.dropout = dropout\n        self.num_classes = num_classes\n        self.representation_size = representation_size\n        self.norm_layer = norm_layer\n\n        self.conv_proj = nn.Conv2d(\n            in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n        )\n        \n        h = w = image_size // patch_size \n\n        seq_length = (image_size // patch_size) ** 2\n        self.pos_embedding = nn.Parameter(torch.randn(1, seq_length, hidden_dim) * 0.02, requires_grad=True)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim) * 0.02, requires_grad=True)\n\n        self.encoder = Encoder(\n            num_layers,\n            num_heads,\n            hidden_dim,\n            mlp_dim,\n            dropout,\n            attention_dropout,\n            norm_layer,\n        )\n        \n        self.seq_length = seq_length\n\n        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n        if representation_size is None:\n            heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n        else:\n            heads_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n            heads_layers[\"act\"] = nn.Tanh()\n            heads_layers[\"head\"] = nn.Linear(representation_size, num_classes)\n\n        self.heads = nn.Sequential(heads_layers)\n        \n        # Init the patchify stem\n        fan_in = self.conv_proj.in_channels * self.conv_proj.kernel_size[0] * self.conv_proj.kernel_size[1] // self.conv_proj.groups\n        jax_lecun_normal(self.conv_proj, fan_in)\n\n        if hasattr(self.heads, \"pre_logits\") and isinstance(self.heads.pre_logits, nn.Linear):\n            fan_in = self.heads.pre_logits.in_features\n            jax_lecun_normal(self.heads.pre_logits, fan_in)\n\n        if isinstance(self.heads.head, nn.Linear):\n            nn.init.zeros_(self.heads.head.weight)\n            nn.init.zeros_(self.heads.head.bias)\n\n    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n        n, c, h, w = x.shape\n        p = self.patch_size\n        torch._assert(h == self.image_size, f\"Wrong image height! Expected {self.image_size} but got {h}!\")\n        torch._assert(w == self.image_size, f\"Wrong image width! Expected {self.image_size} but got {w}!\")\n        n_h = h // p\n        n_w = w // p\n\n        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n        x = self.conv_proj(x)\n        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n        x = x.reshape(n, self.hidden_dim, n_h * n_w)\n\n        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n        # The self attention layer expects inputs in the format (N, S, E)\n        # where S is the source sequence length, N is the batch size, E is the\n        # embedding dimension\n        x = x.permute(0, 2, 1)\n\n        return x\n    \n    def forward(self, x: torch.Tensor):\n        # Reshape and permute the input tensor\n        x = self._process_input(x)\n        n = x.shape[0]\n\n        # Add positional embedding\n        x = x + self.pos_embedding\n\n        # Prepend the CLS token\n        cls_token = self.cls_token.expand(n, -1, -1)\n        x = torch.cat([cls_token, x], dim=1)\n\n        # Pass through the encoder\n        x = self.encoder(x)\n\n        # Use only the CLS token for classification\n        x = x[:, 0]\n\n        # Pass through the classification head\n        x = self.heads(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-10-27T02:06:36.707195Z","iopub.execute_input":"2024-10-27T02:06:36.707595Z","iopub.status.idle":"2024-10-27T02:06:36.749049Z","shell.execute_reply.started":"2024-10-27T02:06:36.707555Z","shell.execute_reply":"2024-10-27T02:06:36.748084Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def weight_decay_param(n, p):\n    return p.ndim >= 2 and n.endswith('weight')\n\n# create model\nmodel = SimpleVisionTransformer(\n    image_size=224,\n    patch_size=16,\n    num_layers=12,\n    num_heads=6,\n    hidden_dim=384,\n    mlp_dim=1536,\n)\nmodel = nn.DataParallel(model)\nmodel.to('cuda')\n\nwd_params = [p for n, p in model.named_parameters() if weight_decay_param(n, p) and p.requires_grad]\nnon_wd_params = [p for n, p in model.named_parameters() if not weight_decay_param(n, p) and p.requires_grad]\noriginal_model = model\n\nweight_decay = 0.05\nlearning_rate = 1e-3\n\n# Label smoothing loss\ncriterion = LabelSmoothing(smoothing=0.1)\n\noptimizer = torch.optim.AdamW(\n    [\n        {\"params\": wd_params, \"weight_decay\": weight_decay},\n        {\"params\": non_wd_params, \"weight_decay\": weight_decay},\n    ],\n    lr=learning_rate,\n    betas=(0.9, 0.999)  # Set beta1=0.9 and beta2=0.999\n)\n\nwarmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: step / warmup_try)\ncosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps - warmup_try)\nscheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, [warmup, cosine], [warmup_try])","metadata":{"execution":{"iopub.status.busy":"2024-10-27T02:08:21.583390Z","iopub.execute_input":"2024-10-27T02:08:21.584222Z","iopub.status.idle":"2024-10-27T02:08:22.146389Z","shell.execute_reply.started":"2024-10-27T02:08:21.584183Z","shell.execute_reply":"2024-10-27T02:08:22.145627Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Change_path_for_the_directory;This is the directory where model weights are to be saved\ncheckpoint_path = \"/kaggle/working/\"\n\ndef save_checkpoint(state, is_best, path, filename='imagenet_baseline_patchconvcheckpoint.pth.tar'):\n    filename = os.path.join(path, filename)\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, os.path.join(path, 'model_best.pth.tar'))\n\ndef save_checkpoint_step(step, model, best_acc1, optimizer, scheduler, checkpoint_path):\n    # Define the filename with the current step\n    filename = os.path.join(checkpoint_path, f'BaseLine_with_PE.pt')\n    \n    # Save the checkpoint\n    torch.save({\n        'step': step,\n        'state_dict': model.state_dict(),\n        'best_acc1': best_acc1,\n        'optimizer': optimizer.state_dict(),\n        'scheduler': scheduler.state_dict()\n    }, filename)\n    \n\nclass Summary(Enum):\n    NONE = 0\n    AVERAGE = 1\n    SUM = 2\n    COUNT = 3\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f', summary_type=Summary.AVERAGE):\n        self.name = name\n        self.fmt = fmt\n        self.summary_type = summary_type\n        self.reset()\n        \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def all_reduce(self):\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda\")\n        elif torch.backends.mps.is_available():\n            device = torch.device(\"mps\")\n        else:\n            device = torch.device(\"cpu\")\n        total = torch.tensor([self.sum, self.count], dtype=torch.float32, device=device)\n        dist.all_reduce(total, dist.ReduceOp.SUM, async_op=False)\n        self.sum, self.count = total.tolist()\n        self.avg = self.sum / self.count\n    \n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n    \n    def summary(self):\n        fmtstr = ''\n        if self.summary_type is Summary.NONE:\n            fmtstr = ''\n        elif self.summary_type is Summary.AVERAGE:\n            fmtstr = '{name} {avg:.3f}'\n        elif self.summary_type is Summary.SUM:\n            fmtstr = '{name} {sum:.3f}'\n        elif self.summary_type is Summary.COUNT:\n            fmtstr = '{name} {count:.3f}'\n        else:\n            raise ValueError('invalid summary type %r' % self.summary_type)\n        \n        return fmtstr.format(**self.__dict__)\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n        \n    def display_summary(self):\n        entries = [\" *\"]\n        entries += [meter.summary() for meter in self.meters]\n        print(' '.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '/' + fmt.format(num_batches) + ']'","metadata":{"execution":{"iopub.status.busy":"2024-10-27T02:08:23.732783Z","iopub.execute_input":"2024-10-27T02:08:23.733152Z","iopub.status.idle":"2024-10-27T02:08:23.752928Z","shell.execute_reply.started":"2024-10-27T02:08:23.733115Z","shell.execute_reply":"2024-10-27T02:08:23.751925Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"log_steps = 2500\n\nwandb.login(key=\"40845bd710604d2ba6b7cdc034459067ea7ae3b8\")\n\n# Initialize a new run\nwandb.init(project=\"ICLR_2025_Blog\", name=\"BaseLine_with_PE\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T02:08:27.531048Z","iopub.execute_input":"2024-10-27T02:08:27.531761Z","iopub.status.idle":"2024-10-27T02:08:32.348858Z","shell.execute_reply.started":"2024-10-27T02:08:27.531723Z","shell.execute_reply":"2024-10-27T02:08:32.347893Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_step = 0","metadata":{"execution":{"iopub.status.busy":"2024-10-27T02:08:38.690574Z","iopub.execute_input":"2024-10-27T02:08:38.690971Z","iopub.status.idle":"2024-10-27T02:08:38.696084Z","shell.execute_reply.started":"2024-10-27T02:08:38.690933Z","shell.execute_reply":"2024-10-27T02:08:38.695220Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\ndef validate(val_loader, model, criterion, step, use_wandb=False, print_freq=100):\n    batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n    losses = AverageMeter('Loss', ':.4e', Summary.NONE)\n    top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n    top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n    progress = ProgressMeter(\n        len(val_loader),\n        [batch_time, losses, top1, top5],\n        prefix='Test: ')\n\n    # switch to evaluate mode\n    model.eval()\n\n    with torch.no_grad():\n        end = time.time()\n        for i, (images, target) in enumerate(val_loader):\n            if torch.cuda.is_available():\n                images = images.cuda(non_blocking=True)\n                target = target.cuda(non_blocking=True)\n            elif torch.backends.mps.is_available():\n                images = images.to('mps')\n                target = target.to('mps')\n\n            # compute output\n            output = model(images)\n            loss = criterion(output, target)\n\n            # measure accuracy and record loss\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            losses.update(loss.item(), images.size(0))\n            top1.update(acc1[0], images.size(0))\n            top5.update(acc5[0], images.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % print_freq == 0:\n                progress.display(i)\n\n    progress.display_summary()\n    \n    if use_wandb:        \n        log_data = {\n            'val/loss': losses.avg,\n            'val/acc@1': top1.avg,\n            'val/acc@5': top5.avg,\n        }\n        wandb.log(log_data, step=step)\n\n    return top1.avg\n\ndef train(train_loader, val_loader, start_step, total_steps, original_model, model, criterion, optimizer, scheduler, device):\n    \n    def load_checkpoint(checkpoint_path, model, optimizer, scheduler):\n        print(f\"Loading checkpoint from {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path)\n        start_step = checkpoint['step']\n        model.load_state_dict(checkpoint['state_dict'])\n        best_acc1 = checkpoint['best_acc1']\n        optimizer.load_state_dict(checkpoint['optimizer'])\n        scheduler.load_state_dict(checkpoint['scheduler'])\n        print(f\"Loaded checkpoint. Resuming from step {start_step}\")\n        return start_step, best_acc1\n    \n    # Load checkpoint\n    start_step, best_acc1 = load_checkpoint(\"/kaggle/input/mmmmmmttttttt/BaseLine_with_PE.pt\", original_model, optimizer, scheduler)\n    \n    batch_time = AverageMeter('Time', ':6.3f')\n    data_time = AverageMeter('Data', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    print_freq = 100\n    log_steps = 2500\n    \n    progress = ProgressMeter(\n        total_steps,\n        [batch_time, data_time, losses, top1, top5]\n    )\n\n    model.train()\n    end = time.time()\n    \n    def infinite_loader():\n        while True:\n            yield from train_loader\n            \n    for step, (images, labels_a, labels_b, lam) in zip(range(start_step + 1, total_steps + 1), infinite_loader()):\n        \n        print(step)\n        \n        data_time.update(time.time() - end)\n\n        images = images.to(device, non_blocking=True)\n        labels_a = labels_a.to(device, non_blocking=True)\n        labels_b = labels_b.to(device, non_blocking=True)\n        \n        # Convert lam to a tensor if it's not already one\n        if not isinstance(lam, torch.Tensor):\n            lam = torch.tensor(lam, device=device)\n        else:\n            lam = lam.to(device, non_blocking=True)\n\n        output = model(images)\n        loss = lam * criterion(output, labels_a) + (1 - lam) * criterion(output, labels_b)\n\n        # Compute accuracy (this is an approximation for mixed labels)\n        acc1_a, acc5_a = accuracy(output, labels_a, topk=(1, 5))\n        acc1_b, acc5_b = accuracy(output, labels_b, topk=(1, 5))\n        acc1 = lam * acc1_a + (1 - lam) * acc1_b\n        acc5 = lam * acc5_a + (1 - lam) * acc5_b\n\n        losses.update(loss.item(), images.size(0))\n        top1.update(acc1[0].item(), images.size(0))\n        top5.update(acc5[0].item(), images.size(0))\n\n        loss.backward()\n        l2_grads = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        if step % print_freq == 0:\n            progress.display(step)\n            if wandb:\n                with torch.no_grad():\n                    l2_params = sum(p.square().sum().item() for _, p in model.named_parameters())\n                    \n                samples_per_second_per_gpu = images.size(0) / batch_time.val\n                samples_per_second = samples_per_second_per_gpu \n                log_data = {\n                    \"train/loss\": losses.val,\n                    'train/acc@1': top1.val,\n                    'train/acc@5': top5.val,\n                    \"data_time\": data_time.val,\n                    \"batch_time\": batch_time.val,\n                    \"samples_per_second\": samples_per_second,\n                    \"samples_per_second_per_gpu\": samples_per_second_per_gpu,\n                    \"lr\": scheduler.get_last_lr()[0],\n                    \"l2_grads\": l2_grads.item(),\n                    \"l2_params\": math.sqrt(l2_params)\n                }\n                wandb.log(log_data, step=step)\n        \n        if ((step % print_freq == 0) and ((step % log_steps != 0) and (step != total_steps))):        \n            save_checkpoint_step(step, model, best_acc1, optimizer, scheduler, checkpoint_path)\n                \n        if step % log_steps == 0:\n            acc1 = validate(val_loader, original_model, criterion, step)\n            is_best = acc1 > best_acc1\n            best_acc1 = max(acc1, best_acc1)\n            \n            save_checkpoint({\n                'step': step,\n                'state_dict': original_model.state_dict(),\n                'best_acc1': best_acc1,\n                'optimizer' : optimizer.state_dict(),\n                'scheduler' : scheduler.state_dict()\n            }, is_best, checkpoint_path)\n            \n        elif step == total_steps:\n            acc1 = validate(val_loader, original_model, criterion, step)\n            is_best = acc1 > best_acc1\n            best_acc1 = max(acc1, best_acc1)\n            \n            save_checkpoint({\n                'step': step,\n                'state_dict': original_model.state_dict(),\n                'best_acc1': best_acc1,\n                'optimizer' : optimizer.state_dict(),\n                'scheduler' : scheduler.state_dict()\n            }, is_best, checkpoint_path)\n\n        scheduler.step()\n\n# Use the modified train function\ntrain(train_loader, val_loader, start_step, total_steps, original_model, model, criterion, optimizer, scheduler, device)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T02:22:12.728587Z","iopub.execute_input":"2024-10-27T02:22:12.728988Z","iopub.status.idle":"2024-10-27T02:31:15.524632Z","shell.execute_reply.started":"2024-10-27T02:22:12.728950Z","shell.execute_reply":"2024-10-27T02:31:15.523339Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T07:26:25.732942Z","iopub.execute_input":"2024-10-19T07:26:25.733234Z","iopub.status.idle":"2024-10-19T07:26:26.073142Z","shell.execute_reply.started":"2024-10-19T07:26:25.733201Z","shell.execute_reply":"2024-10-19T07:26:26.071968Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Test dataset paths first\n# print(\"=== Testing Dataset Paths ===\")\n# for path in train_dirs + val_dir:\n#     print(f\"Checking path {path}: {os.path.exists(path)}\")\n# print(f\"Checking labels file {labels_file}: {os.path.exists(labels_file)}\")\n\n# # Test label loading\n# print(\"\\n=== Testing Label Loading ===\")\n# with open(labels_file, 'r') as f:\n#     label_dict = json.load(f)\n# print(\"Number of unique labels:\", len(label_dict))\n# print(\"First few labels:\", list(label_dict.keys())[:5])\n\n# # Test dataset creation\n# print(\"\\n=== Testing Dataset Creation ===\")\n# try:\n#     test_dataset = ImageNet100Dataset(\n#         root_dirs=train_dirs,\n#         labels_file=labels_file,\n#         transform=train_transform\n#     )\n#     print(\"Training dataset size:\", len(test_dataset))\n    \n#     # Test single item loading\n#     img, label = test_dataset[0]\n#     print(\"Single image shape:\", img.shape)\n#     print(\"Single label:\", label)\n    \n#     # Visualize transformations on one image\n#     plt.figure(figsize=(15, 3))\n    \n#     # Original image\n#     img_orig = Image.open(test_dataset.images[0]).convert('RGB')\n#     plt.subplot(1, 4, 1)\n#     plt.imshow(img_orig)\n#     plt.title(\"Original\")\n    \n#     # Apply transformations 3 times to show randomness\n#     for i in range(3):\n#         img_transformed, _ = test_dataset[0]\n#         plt.subplot(1, 4, i+2)\n#         plt.imshow(img_transformed.permute(1, 2, 0).clip(0, 1))\n#         plt.title(f\"Transform {i+1}\")\n    \n#     plt.show()\n    \n# except Exception as e:\n#     print(\"Error in dataset creation:\", str(e))\n\n# # Test dataloader\n# print(\"\\n=== Testing DataLoader ===\")\n# try:\n#     test_loader = torch.utils.data.DataLoader(\n#         test_dataset,\n#         batch_size=batch_size,\n#         shuffle=True,\n#         collate_fn=collate_fn,\n#         num_workers=2\n#     )\n    \n#     # Get one batch\n#     batch = next(iter(test_loader))\n#     print(\"Batch length:\", len(batch))  # Should be 4 (images, labels_a, labels_b, lam)\n#     images, labels_a, labels_b, lam = batch\n#     print(\"Batch shapes:\")\n#     print(f\"Images: {images.shape}\")\n#     print(f\"Labels A: {labels_a.shape}\")\n#     print(f\"Labels B: {labels_b.shape}\")\n#     print(f\"Lambda: {lam}\")\n    \n# except Exception as e:\n#     print(\"Error in dataloader:\", str(e))\n    \n# print(\"=== Testing Model ===\")\n\n# # Test model creation\n# try:\n#     test_model = SimpleVisionTransformer(\n#         image_size=256,\n#         patch_size=16,\n#         num_layers=12,\n#         num_heads=6,\n#         hidden_dim=384,\n#         mlp_dim=1536,\n#     )\n#     print(\"Model created successfully\")\n    \n#     # Print model summary\n#     print(\"\\nModel Architecture:\")\n#     print(test_model)\n    \n#     # Count parameters\n#     total_params = sum(p.numel() for p in test_model.parameters())\n#     trainable_params = sum(p.numel() for p in test_model.parameters() if p.requires_grad)\n#     print(f\"\\nTotal parameters: {total_params:,}\")\n#     print(f\"Trainable parameters: {trainable_params:,}\")\n    \n#     # Test forward pass\n#     print(\"\\nTesting forward pass...\")\n#     test_model.to(device)\n#     test_input = torch.randn(2, 3, 256, 256).to(device)  # batch size of 2\n#     with torch.no_grad():\n#         output = test_model(test_input)\n#     print(\"Output shape:\", output.shape)\n    \n#     # Test memory usage\n#     if torch.cuda.is_available():\n#         print(\"\\nGPU Memory Usage:\")\n#         print(f\"Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n#         print(f\"Cached: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n    \n# except Exception as e:\n#     print(\"Error in model testing:\", str(e))\n    \n# print(\"=== Testing Loss Function and Optimizer ===\")\n\n# try:\n#     # Test loss function\n#     print(\"\\nTesting Label Smoothing Loss...\")\n#     test_criterion = LabelSmoothing(smoothing=0.1)\n#     test_predictions = torch.randn(4, 100)  # 4 samples, 100 classes\n#     test_targets = torch.tensor([0, 1, 2, 3])\n#     test_loss = test_criterion(test_predictions, test_targets)\n#     print(\"Test loss value:\", test_loss.item())\n    \n#     # Test with different smoothing values\n#     for smoothing in [0.0, 0.1, 0.2]:\n#         criterion = LabelSmoothing(smoothing=smoothing)\n#         loss = criterion(test_predictions, test_targets)\n#         print(f\"Loss with smoothing {smoothing}: {loss.item()}\")\n    \n#     # Test optimizer\n#     print(\"\\nTesting Optimizer...\")\n#     test_model = SimpleVisionTransformer(\n#         image_size=256,\n#         patch_size=16,\n#         num_layers=12,\n#         num_heads=6,\n#         hidden_dim=384,\n#         mlp_dim=1536,\n#     ).to(device)\n    \n#     # Test parameter grouping\n#     wd_params = [p for n, p in test_model.named_parameters() if weight_decay_param(n, p) and p.requires_grad]\n#     non_wd_params = [p for n, p in test_model.named_parameters() if not weight_decay_param(n, p) and p.requires_grad]\n#     print(f\"Parameters with weight decay: {len(wd_params)}\")\n#     print(f\"Parameters without weight decay: {len(non_wd_params)}\")\n    \n#     # Test optimizer creation\n#     test_optimizer = torch.optim.AdamW(\n#         [\n#             {\"params\": wd_params, \"weight_decay\": weight_decay},\n#             {\"params\": non_wd_params, \"weight_decay\": 0.0},\n#         ],\n#         lr=learning_rate,\n#     )\n#     print(\"Optimizer created successfully\")\n    \n#     # Test scheduler\n#     print(\"\\nTesting Learning Rate Scheduler...\")\n#     test_warmup = torch.optim.lr_scheduler.LambdaLR(test_optimizer, lr_lambda=lambda step: step / warmup_try)\n#     test_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(test_optimizer, T_max=total_steps - warmup_try)\n#     test_scheduler = torch.optim.lr_scheduler.SequentialLR(test_optimizer, [test_warmup, test_cosine], [warmup_try])\n    \n#     # Print learning rates at different steps\n#     steps_to_check = [0, warmup_try//2, warmup_try, total_steps//2, total_steps]\n#     print(\"\\nLearning rate at different steps:\")\n#     for step in steps_to_check:\n#         for _ in range(step):\n#             test_scheduler.step()\n#         print(f\"Step {step}: {test_scheduler.get_last_lr()[0]}\")\n    \n# except Exception as e:\n#     print(\"Error in loss/optimizer testing:\", str(e))\n    \n# print(\"=== Testing Training Setup ===\")\n\n# # Test CUDA availability\n# print(\"\\nTesting CUDA Setup:\")\n# print(f\"CUDA available: {torch.cuda.is_available()}\")\n# print(f\"Current device: {device}\")\n# if torch.cuda.is_available():\n#     print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n\n# # Test Wandb initialization\n# print(\"\\nTesting Wandb Setup...\")\n# try:\n#     wandb.init(project=\"ICLR_2025_Blog\", name=\"test_run\")\n#     print(\"Wandb initialized successfully\")\n    \n#     # Test logging\n#     wandb.log({\n#         \"test_metric\": 0.5,\n#         \"test_loss\": 0.1\n#     })\n#     print(\"Wandb logging successful\")\n#     wandb.finish()\n# except Exception as e:\n#     print(\"Error in wandb setup:\", str(e))\n\n# # Test training loop components\n# print(\"\\nTesting Training Loop Components...\")\n# try:\n#     # Test progress meter\n#     batch_time = AverageMeter('Time', ':6.3f')\n#     data_time = AverageMeter('Data', ':6.3f')\n#     losses = AverageMeter('Loss', ':.4e')\n#     top1 = AverageMeter('Acc@1', ':6.2f')\n#     progress = ProgressMeter(\n#         total_steps,\n#         [batch_time, data_time, losses, top1],\n#         prefix=\"Test: \"\n#     )\n    \n#     # Update meters\n#     batch_time.update(0.5)\n#     losses.update(0.1)\n#     top1.update(95.0)\n    \n#     # Test display\n#     progress.display(0)\n    \n#     # Test checkpoint saving\n#     print(\"\\nTesting Checkpoint Saving...\")\n#     dummy_state = {\n#         'step': 0,\n#         'state_dict': test_model.state_dict(),\n#         'best_acc1': 0,\n#         'optimizer': test_optimizer.state_dict(),\n#         'scheduler': test_scheduler.state_dict()\n#     }\n#     save_checkpoint(dummy_state, False, checkpoint_path)\n#     print(f\"Checkpoint saved to {checkpoint_path}\")\n    \n# except Exception as e:\n#     print(\"Error in training loop components:\", str(e))\n\n# print(\"\\nAll tests completed!\")","metadata":{},"outputs":[],"execution_count":null}]}