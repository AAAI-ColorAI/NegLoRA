{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de99ebe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:30:42.291879Z",
     "iopub.status.busy": "2024-11-22T21:30:42.291553Z",
     "iopub.status.idle": "2024-11-22T23:32:28.559999Z",
     "shell.execute_reply": "2024-11-22T23:32:28.559344Z"
    },
    "papermill": {
     "duration": 7306.274291,
     "end_time": "2024-11-22T23:32:28.561690",
     "exception": false,
     "start_time": "2024-11-22T21:30:42.287399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169001437/169001437 [00:04<00:00, 35178594.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33matharv_m\u001b[0m (\u001b[33matharv_m-iit-roorkee\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20241122_213057-sfgxihe6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mBaseline_ImageNet_run\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/atharv_m-iit-roorkee/fractual_transformer\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/atharv_m-iit-roorkee/fractual_transformer/runs/sfgxihe6\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[  100/35156]\tTime  0.204 ( 0.215)\tData  0.000 ( 0.003)\tLoss 4.5854e+00 (4.5968e+00)\tAcc@1   0.02 (  0.03)\tAcc@5   0.11 (  0.13)\n",
      "200\n",
      "[  200/35156]\tTime  0.204 ( 0.210)\tData  0.000 ( 0.002)\tLoss 4.4474e+00 (4.5642e+00)\tAcc@1   0.08 (  0.04)\tAcc@5   0.23 (  0.15)\n",
      "300\n",
      "[  300/35156]\tTime  0.204 ( 0.208)\tData  0.000 ( 0.002)\tLoss 4.4273e+00 (4.5165e+00)\tAcc@1   0.05 (  0.04)\tAcc@5   0.16 (  0.16)\n",
      "400\n",
      "[  400/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.002)\tLoss 4.2234e+00 (4.4605e+00)\tAcc@1   0.08 (  0.04)\tAcc@5   0.24 (  0.18)\n",
      "500\n",
      "[  500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.002)\tLoss 4.1439e+00 (4.4006e+00)\tAcc@1   0.05 (  0.05)\tAcc@5   0.30 (  0.20)\n",
      "600\n",
      "[  600/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.002)\tLoss 3.9667e+00 (4.3433e+00)\tAcc@1   0.09 (  0.06)\tAcc@5   0.30 (  0.21)\n",
      "700\n",
      "[  700/35156]\tTime  0.202 ( 0.205)\tData  0.000 ( 0.002)\tLoss 3.8297e+00 (4.2879e+00)\tAcc@1   0.15 (  0.06)\tAcc@5   0.37 (  0.23)\n",
      "800\n",
      "[  800/35156]\tTime  0.204 ( 0.205)\tData  0.000 ( 0.002)\tLoss 3.5766e+00 (4.2405e+00)\tAcc@1   0.17 (  0.07)\tAcc@5   0.46 (  0.24)\n",
      "900\n",
      "[  900/35156]\tTime  0.204 ( 0.205)\tData  0.000 ( 0.002)\tLoss 4.1218e+00 (4.1926e+00)\tAcc@1   0.09 (  0.08)\tAcc@5   0.34 (  0.26)\n",
      "1000\n",
      "[ 1000/35156]\tTime  0.204 ( 0.205)\tData  0.000 ( 0.002)\tLoss 4.1859e+00 (4.1484e+00)\tAcc@1   0.09 (  0.08)\tAcc@5   0.30 (  0.27)\n",
      "1100\n",
      "[ 1100/35156]\tTime  0.203 ( 0.205)\tData  0.000 ( 0.002)\tLoss 4.1148e+00 (4.1070e+00)\tAcc@1   0.08 (  0.09)\tAcc@5   0.30 (  0.29)\n",
      "1200\n",
      "[ 1200/35156]\tTime  0.203 ( 0.205)\tData  0.000 ( 0.002)\tLoss 3.2944e+00 (4.0587e+00)\tAcc@1   0.25 (  0.10)\tAcc@5   0.58 (  0.30)\n",
      "1300\n",
      "[ 1300/35156]\tTime  0.204 ( 0.205)\tData  0.000 ( 0.002)\tLoss 3.2560e+00 (4.0153e+00)\tAcc@1   0.23 (  0.10)\tAcc@5   0.48 (  0.32)\n",
      "1400\n",
      "[ 1400/35156]\tTime  0.204 ( 0.205)\tData  0.000 ( 0.002)\tLoss 3.3503e+00 (3.9713e+00)\tAcc@1   0.20 (  0.11)\tAcc@5   0.51 (  0.33)\n",
      "1500\n",
      "[ 1500/35156]\tTime  0.204 ( 0.205)\tData  0.000 ( 0.002)\tLoss 3.2151e+00 (3.9279e+00)\tAcc@1   0.14 (  0.12)\tAcc@5   0.48 (  0.34)\n",
      "1600\n",
      "[ 1600/35156]\tTime  0.204 ( 0.205)\tData  0.000 ( 0.002)\tLoss 3.7551e+00 (3.8883e+00)\tAcc@1   0.23 (  0.13)\tAcc@5   0.48 (  0.35)\n",
      "1700\n",
      "[ 1700/35156]\tTime  0.204 ( 0.205)\tData  0.000 ( 0.002)\tLoss 2.9601e+00 (3.8497e+00)\tAcc@1   0.31 (  0.13)\tAcc@5   0.62 (  0.37)\n",
      "1800\n",
      "[ 1800/35156]\tTime  0.203 ( 0.205)\tData  0.000 ( 0.002)\tLoss 3.6357e+00 (3.8158e+00)\tAcc@1   0.20 (  0.14)\tAcc@5   0.55 (  0.38)\n",
      "1900\n",
      "[ 1900/35156]\tTime  0.203 ( 0.205)\tData  0.000 ( 0.002)\tLoss 2.8696e+00 (3.7830e+00)\tAcc@1   0.30 (  0.14)\tAcc@5   0.56 (  0.39)\n",
      "2000\n",
      "[ 2000/35156]\tTime  0.204 ( 0.205)\tData  0.000 ( 0.002)\tLoss 2.7774e+00 (3.7474e+00)\tAcc@1   0.30 (  0.15)\tAcc@5   0.62 (  0.40)\n",
      "2100\n",
      "[ 2100/35156]\tTime  0.204 ( 0.205)\tData  0.000 ( 0.002)\tLoss 3.0683e+00 (3.7181e+00)\tAcc@1   0.23 (  0.16)\tAcc@5   0.58 (  0.41)\n",
      "2200\n",
      "[ 2200/35156]\tTime  0.205 ( 0.205)\tData  0.000 ( 0.002)\tLoss 3.1258e+00 (3.6881e+00)\tAcc@1   0.20 (  0.16)\tAcc@5   0.55 (  0.41)\n",
      "2300\n",
      "[ 2300/35156]\tTime  0.204 ( 0.205)\tData  0.000 ( 0.002)\tLoss 2.8268e+00 (3.6625e+00)\tAcc@1   0.24 (  0.17)\tAcc@5   0.62 (  0.42)\n",
      "2400\n",
      "[ 2400/35156]\tTime  0.204 ( 0.205)\tData  0.000 ( 0.002)\tLoss 3.4307e+00 (3.6352e+00)\tAcc@1   0.22 (  0.17)\tAcc@5   0.60 (  0.43)\n",
      "2500\n",
      "[ 2500/35156]\tTime  0.204 ( 0.204)\tData  0.000 ( 0.002)\tLoss 2.8682e+00 (3.6121e+00)\tAcc@1   0.27 (  0.18)\tAcc@5   0.64 (  0.44)\n",
      "Test: [ 0/79]\tTime  0.242 ( 0.242)\tLoss 2.6656e+00 (2.6656e+00)\tAcc@1   0.34 (  0.34)\tAcc@5   0.59 (  0.59)\n",
      " *   Acc@1 0.314 Acc@5 0.630\n",
      "2600\n",
      "[ 2600/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.9052e+00 (3.5869e+00)\tAcc@1   0.30 (  0.18)\tAcc@5   0.62 (  0.44)\n",
      "2700\n",
      "[ 2700/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.5322e+00 (3.5638e+00)\tAcc@1   0.28 (  0.19)\tAcc@5   0.62 (  0.45)\n",
      "2800\n",
      "[ 2800/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.9731e+00 (3.5388e+00)\tAcc@1   0.36 (  0.19)\tAcc@5   0.71 (  0.46)\n",
      "2900\n",
      "[ 2900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.4895e+00 (3.5144e+00)\tAcc@1   0.34 (  0.20)\tAcc@5   0.69 (  0.47)\n",
      "3000\n",
      "[ 3000/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.6983e+00 (3.4906e+00)\tAcc@1   0.36 (  0.20)\tAcc@5   0.62 (  0.47)\n",
      "3100\n",
      "[ 3100/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.6074e+00 (3.4673e+00)\tAcc@1   0.38 (  0.21)\tAcc@5   0.64 (  0.48)\n",
      "3200\n",
      "[ 3200/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.1153e+00 (3.4446e+00)\tAcc@1   0.43 (  0.21)\tAcc@5   0.77 (  0.48)\n",
      "3300\n",
      "[ 3300/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.4724e+00 (3.4233e+00)\tAcc@1   0.40 (  0.22)\tAcc@5   0.70 (  0.49)\n",
      "3400\n",
      "[ 3400/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.7854e+00 (3.4035e+00)\tAcc@1   0.25 (  0.22)\tAcc@5   0.64 (  0.50)\n",
      "3500\n",
      "[ 3500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.8837e+00 (3.3875e+00)\tAcc@1   0.28 (  0.22)\tAcc@5   0.64 (  0.50)\n",
      "3600\n",
      "[ 3600/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.5547e+00 (3.3687e+00)\tAcc@1   0.34 (  0.23)\tAcc@5   0.64 (  0.51)\n",
      "3700\n",
      "[ 3700/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.5883e+00 (3.3501e+00)\tAcc@1   0.34 (  0.23)\tAcc@5   0.64 (  0.51)\n",
      "3800\n",
      "[ 3800/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.1247e+00 (3.3339e+00)\tAcc@1   0.45 (  0.23)\tAcc@5   0.80 (  0.52)\n",
      "3900\n",
      "[ 3900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.5269e+00 (3.3191e+00)\tAcc@1   0.32 (  0.24)\tAcc@5   0.66 (  0.52)\n",
      "4000\n",
      "[ 4000/35156]\tTime  0.213 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.0160e+00 (3.3039e+00)\tAcc@1   0.38 (  0.24)\tAcc@5   0.74 (  0.52)\n",
      "4100\n",
      "[ 4100/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.1449e+00 (3.2869e+00)\tAcc@1   0.44 (  0.25)\tAcc@5   0.73 (  0.53)\n",
      "4200\n",
      "[ 4200/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.003)\tLoss 2.2181e+00 (3.2703e+00)\tAcc@1   0.40 (  0.25)\tAcc@5   0.76 (  0.53)\n",
      "4300\n",
      "[ 4300/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.003)\tLoss 3.9380e+00 (3.2527e+00)\tAcc@1   0.15 (  0.25)\tAcc@5   0.49 (  0.54)\n",
      "4400\n",
      "[ 4400/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.003)\tLoss 2.9249e+00 (3.2351e+00)\tAcc@1   0.48 (  0.26)\tAcc@5   0.75 (  0.54)\n",
      "4500\n",
      "[ 4500/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.003)\tLoss 2.2119e+00 (3.2233e+00)\tAcc@1   0.43 (  0.26)\tAcc@5   0.73 (  0.54)\n",
      "4600\n",
      "[ 4600/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.003)\tLoss 3.8827e+00 (3.2096e+00)\tAcc@1   0.23 (  0.26)\tAcc@5   0.48 (  0.55)\n",
      "4700\n",
      "[ 4700/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.003)\tLoss 3.3938e+00 (3.1950e+00)\tAcc@1   0.35 (  0.26)\tAcc@5   0.66 (  0.55)\n",
      "4800\n",
      "[ 4800/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.003)\tLoss 3.5807e+00 (3.1820e+00)\tAcc@1   0.37 (  0.27)\tAcc@5   0.61 (  0.56)\n",
      "4900\n",
      "[ 4900/35156]\tTime  0.201 ( 0.205)\tData  0.000 ( 0.003)\tLoss 2.1869e+00 (3.1683e+00)\tAcc@1   0.41 (  0.27)\tAcc@5   0.77 (  0.56)\n",
      "5000\n",
      "[ 5000/35156]\tTime  0.203 ( 0.205)\tData  0.000 ( 0.003)\tLoss 2.1314e+00 (3.1541e+00)\tAcc@1   0.43 (  0.27)\tAcc@5   0.77 (  0.56)\n",
      "Test: [ 0/79]\tTime  0.224 ( 0.224)\tLoss 2.1701e+00 (2.1701e+00)\tAcc@1   0.42 (  0.42)\tAcc@5   0.74 (  0.74)\n",
      " *   Acc@1 0.399 Acc@5 0.722\n",
      "5100\n",
      "[ 5100/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.5111e+00 (3.1422e+00)\tAcc@1   0.31 (  0.28)\tAcc@5   0.65 (  0.57)\n",
      "5200\n",
      "[ 5200/35156]\tTime  0.201 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.9812e+00 (3.1305e+00)\tAcc@1   0.40 (  0.28)\tAcc@5   0.77 (  0.57)\n",
      "5300\n",
      "[ 5300/35156]\tTime  0.205 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.7924e+00 (3.1173e+00)\tAcc@1   0.48 (  0.28)\tAcc@5   0.80 (  0.57)\n",
      "5400\n",
      "[ 5400/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.9590e+00 (3.1053e+00)\tAcc@1   0.48 (  0.28)\tAcc@5   0.80 (  0.58)\n",
      "5500\n",
      "[ 5500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.8952e+00 (3.0943e+00)\tAcc@1   0.52 (  0.29)\tAcc@5   0.82 (  0.58)\n",
      "5600\n",
      "[ 5600/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.8301e+00 (3.0816e+00)\tAcc@1   0.41 (  0.29)\tAcc@5   0.73 (  0.58)\n",
      "5700\n",
      "[ 5700/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.0109e+00 (3.0672e+00)\tAcc@1   0.54 (  0.29)\tAcc@5   0.85 (  0.59)\n",
      "5800\n",
      "[ 5800/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.3572e+00 (3.0574e+00)\tAcc@1   0.38 (  0.30)\tAcc@5   0.66 (  0.59)\n",
      "5900\n",
      "[ 5900/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.2273e+00 (3.0473e+00)\tAcc@1   0.44 (  0.30)\tAcc@5   0.74 (  0.59)\n",
      "6000\n",
      "[ 6000/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.3186e+00 (3.0332e+00)\tAcc@1   0.48 (  0.30)\tAcc@5   0.82 (  0.59)\n",
      "6100\n",
      "[ 6100/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.3900e+00 (3.0223e+00)\tAcc@1   0.38 (  0.30)\tAcc@5   0.70 (  0.60)\n",
      "6200\n",
      "[ 6200/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.8984e+00 (3.0119e+00)\tAcc@1   0.45 (  0.31)\tAcc@5   0.86 (  0.60)\n",
      "6300\n",
      "[ 6300/35156]\tTime  0.205 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.1771e+00 (2.9997e+00)\tAcc@1   0.57 (  0.31)\tAcc@5   0.84 (  0.60)\n",
      "6400\n",
      "[ 6400/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.8585e+00 (2.9897e+00)\tAcc@1   0.49 (  0.31)\tAcc@5   0.80 (  0.61)\n",
      "6500\n",
      "[ 6500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.4640e+00 (2.9796e+00)\tAcc@1   0.51 (  0.31)\tAcc@5   0.80 (  0.61)\n",
      "6600\n",
      "[ 6600/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.8532e+00 (2.9696e+00)\tAcc@1   0.49 (  0.32)\tAcc@5   0.83 (  0.61)\n",
      "6700\n",
      "[ 6700/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.6907e+00 (2.9569e+00)\tAcc@1   0.20 (  0.32)\tAcc@5   0.55 (  0.61)\n",
      "6800\n",
      "[ 6800/35156]\tTime  0.205 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.7262e+00 (2.9448e+00)\tAcc@1   0.50 (  0.32)\tAcc@5   0.85 (  0.62)\n",
      "6900\n",
      "[ 6900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.9014e+00 (2.9344e+00)\tAcc@1   0.12 (  0.32)\tAcc@5   0.33 (  0.62)\n",
      "7000\n",
      "[ 7000/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.9330e+00 (2.9252e+00)\tAcc@1   0.56 (  0.33)\tAcc@5   0.80 (  0.62)\n",
      "7100\n",
      "[ 7100/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.5467e+00 (2.9156e+00)\tAcc@1   0.55 (  0.33)\tAcc@5   0.89 (  0.62)\n",
      "7200\n",
      "[ 7200/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.0973e+00 (2.9055e+00)\tAcc@1   0.55 (  0.33)\tAcc@5   0.88 (  0.63)\n",
      "7300\n",
      "[ 7300/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.6825e+00 (2.8930e+00)\tAcc@1   0.53 (  0.33)\tAcc@5   0.83 (  0.63)\n",
      "7400\n",
      "[ 7400/35156]\tTime  0.201 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.6375e+00 (2.8832e+00)\tAcc@1   0.52 (  0.34)\tAcc@5   0.85 (  0.63)\n",
      "7500\n",
      "[ 7500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.8028e+00 (2.8707e+00)\tAcc@1   0.28 (  0.34)\tAcc@5   0.61 (  0.63)\n",
      "Test: [ 0/79]\tTime  0.237 ( 0.237)\tLoss 1.7517e+00 (1.7517e+00)\tAcc@1   0.59 (  0.59)\tAcc@5   0.81 (  0.81)\n",
      " *   Acc@1 0.468 Acc@5 0.771\n",
      "7600\n",
      "[ 7600/35156]\tTime  0.205 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.3643e+00 (2.8607e+00)\tAcc@1   0.65 (  0.34)\tAcc@5   0.89 (  0.64)\n",
      "7700\n",
      "[ 7700/35156]\tTime  0.205 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.7649e+00 (2.8517e+00)\tAcc@1   0.57 (  0.34)\tAcc@5   0.82 (  0.64)\n",
      "7800\n",
      "[ 7800/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.0586e+00 (2.8418e+00)\tAcc@1   0.50 (  0.35)\tAcc@5   0.74 (  0.64)\n",
      "7900\n",
      "[ 7900/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.5308e+00 (2.8306e+00)\tAcc@1   0.36 (  0.35)\tAcc@5   0.65 (  0.64)\n",
      "8000\n",
      "[ 8000/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.1561e+00 (2.8213e+00)\tAcc@1   0.67 (  0.35)\tAcc@5   0.94 (  0.65)\n",
      "8100\n",
      "[ 8100/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.3523e+00 (2.8112e+00)\tAcc@1   0.63 (  0.35)\tAcc@5   0.89 (  0.65)\n",
      "8200\n",
      "[ 8200/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.0778e+00 (2.8018e+00)\tAcc@1   0.74 (  0.36)\tAcc@5   0.93 (  0.65)\n",
      "8300\n",
      "[ 8300/35156]\tTime  0.214 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.3688e+00 (2.7909e+00)\tAcc@1   0.59 (  0.36)\tAcc@5   0.91 (  0.65)\n",
      "8400\n",
      "[ 8400/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.5587e+00 (2.7798e+00)\tAcc@1   0.54 (  0.36)\tAcc@5   0.84 (  0.66)\n",
      "8500\n",
      "[ 8500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.4242e+00 (2.7700e+00)\tAcc@1   0.59 (  0.36)\tAcc@5   0.90 (  0.66)\n",
      "8600\n",
      "[ 8600/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.1395e+00 (2.7595e+00)\tAcc@1   0.70 (  0.36)\tAcc@5   0.91 (  0.66)\n",
      "8700\n",
      "[ 8700/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.6399e+00 (2.7486e+00)\tAcc@1   0.58 (  0.37)\tAcc@5   0.88 (  0.66)\n",
      "8800\n",
      "[ 8800/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.7813e+00 (2.7415e+00)\tAcc@1   0.46 (  0.37)\tAcc@5   0.83 (  0.66)\n",
      "8900\n",
      "[ 8900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.6917e+00 (2.7328e+00)\tAcc@1   0.61 (  0.37)\tAcc@5   0.87 (  0.67)\n",
      "9000\n",
      "[ 9000/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.2487e+00 (2.7252e+00)\tAcc@1   0.66 (  0.37)\tAcc@5   0.90 (  0.67)\n",
      "9100\n",
      "[ 9100/35156]\tTime  0.205 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.1375e+00 (2.7147e+00)\tAcc@1   0.73 (  0.38)\tAcc@5   0.95 (  0.67)\n",
      "9200\n",
      "[ 9200/35156]\tTime  0.200 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.2928e+00 (2.7053e+00)\tAcc@1   0.62 (  0.38)\tAcc@5   0.91 (  0.67)\n",
      "9300\n",
      "[ 9300/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.4508e+00 (2.6979e+00)\tAcc@1   0.62 (  0.38)\tAcc@5   0.89 (  0.67)\n",
      "9400\n",
      "[ 9400/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.2719e+00 (2.6897e+00)\tAcc@1   0.62 (  0.38)\tAcc@5   0.91 (  0.68)\n",
      "9500\n",
      "[ 9500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.4460e+00 (2.6790e+00)\tAcc@1   0.44 (  0.39)\tAcc@5   0.74 (  0.68)\n",
      "9600\n",
      "[ 9600/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.2421e+00 (2.6690e+00)\tAcc@1   0.65 (  0.39)\tAcc@5   0.92 (  0.68)\n",
      "9700\n",
      "[ 9700/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.2171e+00 (2.6625e+00)\tAcc@1   0.62 (  0.39)\tAcc@5   0.91 (  0.68)\n",
      "9800\n",
      "[ 9800/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 8.3988e-01 (2.6533e+00)\tAcc@1   0.73 (  0.39)\tAcc@5   0.97 (  0.68)\n",
      "9900\n",
      "[ 9900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.0832e+00 (2.6436e+00)\tAcc@1   0.68 (  0.39)\tAcc@5   0.93 (  0.69)\n",
      "10000\n",
      "[10000/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.3089e+00 (2.6343e+00)\tAcc@1   0.68 (  0.40)\tAcc@5   0.89 (  0.69)\n",
      "Test: [ 0/79]\tTime  0.226 ( 0.226)\tLoss 1.8756e+00 (1.8756e+00)\tAcc@1   0.55 (  0.55)\tAcc@5   0.77 (  0.77)\n",
      " *   Acc@1 0.496 Acc@5 0.793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10100\n",
      "[10100/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.7110e+00 (2.6262e+00)\tAcc@1   0.29 (  0.40)\tAcc@5   0.58 (  0.69)\n",
      "10200\n",
      "[10200/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.7433e+00 (2.6192e+00)\tAcc@1   0.19 (  0.40)\tAcc@5   0.49 (  0.69)\n",
      "10300\n",
      "[10300/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.0965e+00 (2.6109e+00)\tAcc@1   0.71 (  0.40)\tAcc@5   0.95 (  0.69)\n",
      "10400\n",
      "[10400/35156]\tTime  0.210 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.6550e+00 (2.6019e+00)\tAcc@1   0.20 (  0.41)\tAcc@5   0.50 (  0.70)\n",
      "10500\n",
      "[10500/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.2738e+00 (2.5953e+00)\tAcc@1   0.73 (  0.41)\tAcc@5   0.95 (  0.70)\n",
      "10600\n",
      "[10600/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 8.1518e-01 (2.5873e+00)\tAcc@1   0.77 (  0.41)\tAcc@5   0.98 (  0.70)\n",
      "10700\n",
      "[10700/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 9.1377e-01 (2.5799e+00)\tAcc@1   0.76 (  0.41)\tAcc@5   0.97 (  0.70)\n",
      "10800\n",
      "[10800/35156]\tTime  0.202 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.2325e+00 (2.5713e+00)\tAcc@1   0.73 (  0.41)\tAcc@5   0.95 (  0.70)\n",
      "10900\n",
      "[10900/35156]\tTime  0.205 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.3713e+00 (2.5646e+00)\tAcc@1   0.61 (  0.42)\tAcc@5   0.90 (  0.70)\n",
      "11000\n",
      "[11000/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 9.0311e-01 (2.5554e+00)\tAcc@1   0.70 (  0.42)\tAcc@5   0.97 (  0.71)\n",
      "11100\n",
      "[11100/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 9.8284e-01 (2.5457e+00)\tAcc@1   0.73 (  0.42)\tAcc@5   0.96 (  0.71)\n",
      "11200\n",
      "[11200/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 9.7435e-01 (2.5369e+00)\tAcc@1   0.69 (  0.42)\tAcc@5   0.97 (  0.71)\n",
      "11300\n",
      "[11300/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.1822e+00 (2.5281e+00)\tAcc@1   0.42 (  0.42)\tAcc@5   0.74 (  0.71)\n",
      "11400\n",
      "[11400/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.0403e+00 (2.5189e+00)\tAcc@1   0.78 (  0.43)\tAcc@5   0.98 (  0.71)\n",
      "11500\n",
      "[11500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.9143e+00 (2.5096e+00)\tAcc@1   0.70 (  0.43)\tAcc@5   0.95 (  0.71)\n",
      "11600\n",
      "[11600/35156]\tTime  0.211 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.1260e+00 (2.5022e+00)\tAcc@1   0.77 (  0.43)\tAcc@5   0.95 (  0.72)\n",
      "11700\n",
      "[11700/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.3257e+00 (2.4942e+00)\tAcc@1   0.66 (  0.43)\tAcc@5   0.90 (  0.72)\n",
      "11800\n",
      "[11800/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.6648e+00 (2.4830e+00)\tAcc@1   0.80 (  0.44)\tAcc@5   0.98 (  0.72)\n",
      "11900\n",
      "[11900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.0621e+00 (2.4753e+00)\tAcc@1   0.69 (  0.44)\tAcc@5   0.95 (  0.72)\n",
      "12000\n",
      "[12000/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 7.2478e-01 (2.4674e+00)\tAcc@1   0.80 (  0.44)\tAcc@5   0.98 (  0.72)\n",
      "12100\n",
      "[12100/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 7.7909e-01 (2.4578e+00)\tAcc@1   0.84 (  0.44)\tAcc@5   0.98 (  0.73)\n",
      "12200\n",
      "[12200/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.6246e+00 (2.4479e+00)\tAcc@1   0.76 (  0.45)\tAcc@5   0.98 (  0.73)\n",
      "12300\n",
      "[12300/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 8.1147e-01 (2.4397e+00)\tAcc@1   0.82 (  0.45)\tAcc@5   0.97 (  0.73)\n",
      "12400\n",
      "[12400/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.0064e+00 (2.4310e+00)\tAcc@1   0.68 (  0.45)\tAcc@5   0.94 (  0.73)\n",
      "12500\n",
      "[12500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.9259e+00 (2.4218e+00)\tAcc@1   0.55 (  0.45)\tAcc@5   0.92 (  0.73)\n",
      "Test: [ 0/79]\tTime  0.240 ( 0.240)\tLoss 1.6331e+00 (1.6331e+00)\tAcc@1   0.59 (  0.59)\tAcc@5   0.85 (  0.85)\n",
      " *   Acc@1 0.525 Acc@5 0.794\n",
      "12600\n",
      "[12600/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.0339e+00 (2.4128e+00)\tAcc@1   0.82 (  0.46)\tAcc@5   0.98 (  0.73)\n",
      "12700\n",
      "[12700/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.4933e+00 (2.4047e+00)\tAcc@1   0.82 (  0.46)\tAcc@5   1.00 (  0.74)\n",
      "12800\n",
      "[12800/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.3048e+00 (2.3967e+00)\tAcc@1   0.44 (  0.46)\tAcc@5   0.66 (  0.74)\n",
      "12900\n",
      "[12900/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.1968e-01 (2.3885e+00)\tAcc@1   0.92 (  0.46)\tAcc@5   1.00 (  0.74)\n",
      "13000\n",
      "[13000/35156]\tTime  0.202 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.9527e-01 (2.3791e+00)\tAcc@1   0.93 (  0.47)\tAcc@5   0.99 (  0.74)\n",
      "13100\n",
      "[13100/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 5.0764e-01 (2.3698e+00)\tAcc@1   0.88 (  0.47)\tAcc@5   0.99 (  0.74)\n",
      "13200\n",
      "[13200/35156]\tTime  0.202 ( 0.207)\tData  0.000 ( 0.004)\tLoss 4.5489e-01 (2.3609e+00)\tAcc@1   0.91 (  0.47)\tAcc@5   0.99 (  0.74)\n",
      "13300\n",
      "[13300/35156]\tTime  0.205 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.9688e+00 (2.3538e+00)\tAcc@1   0.61 (  0.47)\tAcc@5   0.82 (  0.74)\n",
      "13400\n",
      "[13400/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.5570e-01 (2.3451e+00)\tAcc@1   0.88 (  0.48)\tAcc@5   1.00 (  0.75)\n",
      "13500\n",
      "[13500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 4.1543e-01 (2.3365e+00)\tAcc@1   0.89 (  0.48)\tAcc@5   0.99 (  0.75)\n",
      "13600\n",
      "[13600/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.0877e+00 (2.3286e+00)\tAcc@1   0.55 (  0.48)\tAcc@5   0.78 (  0.75)\n",
      "13700\n",
      "[13700/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.8557e-01 (2.3199e+00)\tAcc@1   0.90 (  0.48)\tAcc@5   0.99 (  0.75)\n",
      "13800\n",
      "[13800/35156]\tTime  0.204 ( 0.206)\tData  0.001 ( 0.004)\tLoss 2.4037e+00 (2.3104e+00)\tAcc@1   0.79 (  0.49)\tAcc@5   0.96 (  0.75)\n",
      "13900\n",
      "[13900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 5.9773e-01 (2.3018e+00)\tAcc@1   0.93 (  0.49)\tAcc@5   1.00 (  0.75)\n",
      "14000\n",
      "[14000/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 5.0541e-01 (2.2940e+00)\tAcc@1   0.82 (  0.49)\tAcc@5   1.00 (  0.75)\n",
      "14100\n",
      "[14100/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.0248e-01 (2.2864e+00)\tAcc@1   0.94 (  0.49)\tAcc@5   0.99 (  0.76)\n",
      "14200\n",
      "[14200/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.2577e+00 (2.2796e+00)\tAcc@1   0.91 (  0.49)\tAcc@5   1.00 (  0.76)\n",
      "14300\n",
      "[14300/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.2992e+00 (2.2723e+00)\tAcc@1   0.91 (  0.50)\tAcc@5   0.99 (  0.76)\n",
      "14400\n",
      "[14400/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 4.0795e-01 (2.2658e+00)\tAcc@1   0.88 (  0.50)\tAcc@5   1.00 (  0.76)\n",
      "14500\n",
      "[14500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.5416e-01 (2.2576e+00)\tAcc@1   0.95 (  0.50)\tAcc@5   1.00 (  0.76)\n",
      "14600\n",
      "[14600/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.6541e-01 (2.2504e+00)\tAcc@1   0.96 (  0.50)\tAcc@5   1.00 (  0.76)\n",
      "14700\n",
      "[14700/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.2501e+00 (2.2422e+00)\tAcc@1   0.78 (  0.51)\tAcc@5   0.96 (  0.76)\n",
      "14800\n",
      "[14800/35156]\tTime  0.205 ( 0.206)\tData  0.000 ( 0.004)\tLoss 5.8810e-01 (2.2350e+00)\tAcc@1   0.90 (  0.51)\tAcc@5   1.00 (  0.76)\n",
      "14900\n",
      "[14900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.5369e-01 (2.2271e+00)\tAcc@1   0.95 (  0.51)\tAcc@5   1.00 (  0.77)\n",
      "15000\n",
      "[15000/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.9602e-01 (2.2190e+00)\tAcc@1   0.96 (  0.51)\tAcc@5   1.00 (  0.77)\n",
      "Test: [ 0/79]\tTime  0.226 ( 0.226)\tLoss 1.8293e+00 (1.8293e+00)\tAcc@1   0.59 (  0.59)\tAcc@5   0.81 (  0.81)\n",
      " *   Acc@1 0.524 Acc@5 0.786\n",
      "15100\n",
      "[15100/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.9909e-01 (2.2110e+00)\tAcc@1   0.96 (  0.52)\tAcc@5   1.00 (  0.77)\n",
      "15200\n",
      "[15200/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.2803e+00 (2.2035e+00)\tAcc@1   0.79 (  0.52)\tAcc@5   0.97 (  0.77)\n",
      "15300\n",
      "[15300/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 6.4795e-01 (2.1958e+00)\tAcc@1   0.96 (  0.52)\tAcc@5   1.00 (  0.77)\n",
      "15400\n",
      "[15400/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.3560e-01 (2.1888e+00)\tAcc@1   0.98 (  0.52)\tAcc@5   1.00 (  0.77)\n",
      "15500\n",
      "[15500/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.6937e+00 (2.1813e+00)\tAcc@1   0.90 (  0.52)\tAcc@5   0.97 (  0.77)\n",
      "15600\n",
      "[15600/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 6.4721e-01 (2.1732e+00)\tAcc@1   0.94 (  0.53)\tAcc@5   1.00 (  0.77)\n",
      "15700\n",
      "[15700/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.9703e-01 (2.1653e+00)\tAcc@1   0.96 (  0.53)\tAcc@5   0.99 (  0.78)\n",
      "15800\n",
      "[15800/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.1832e-01 (2.1577e+00)\tAcc@1   0.99 (  0.53)\tAcc@5   1.00 (  0.78)\n",
      "15900\n",
      "[15900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.2803e-01 (2.1500e+00)\tAcc@1   0.96 (  0.53)\tAcc@5   1.00 (  0.78)\n",
      "16000\n",
      "[16000/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.1866e+00 (2.1434e+00)\tAcc@1   0.95 (  0.54)\tAcc@5   0.98 (  0.78)\n",
      "16100\n",
      "[16100/35156]\tTime  0.205 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.3468e-01 (2.1353e+00)\tAcc@1   0.96 (  0.54)\tAcc@5   1.00 (  0.78)\n",
      "16200\n",
      "[16200/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.3941e+00 (2.1286e+00)\tAcc@1   0.82 (  0.54)\tAcc@5   0.96 (  0.78)\n",
      "16300\n",
      "[16300/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.7531e+00 (2.1213e+00)\tAcc@1   0.89 (  0.54)\tAcc@5   1.00 (  0.78)\n",
      "16400\n",
      "[16400/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.4597e+00 (2.1146e+00)\tAcc@1   0.95 (  0.54)\tAcc@5   1.00 (  0.78)\n",
      "16500\n",
      "[16500/35156]\tTime  0.198 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.4400e+00 (2.1080e+00)\tAcc@1   0.76 (  0.55)\tAcc@5   0.94 (  0.79)\n",
      "16600\n",
      "[16600/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 4.8328e-01 (2.1011e+00)\tAcc@1   0.98 (  0.55)\tAcc@5   1.00 (  0.79)\n",
      "16700\n",
      "[16700/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.5166e+00 (2.0949e+00)\tAcc@1   0.92 (  0.55)\tAcc@5   0.99 (  0.79)\n",
      "16800\n",
      "[16800/35156]\tTime  0.205 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.1614e-01 (2.0880e+00)\tAcc@1   1.00 (  0.55)\tAcc@5   1.00 (  0.79)\n",
      "16900\n",
      "[16900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 9.0523e-01 (2.0800e+00)\tAcc@1   0.98 (  0.55)\tAcc@5   1.00 (  0.79)\n",
      "17000\n",
      "[17000/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.0326e+00 (2.0742e+00)\tAcc@1   0.98 (  0.56)\tAcc@5   1.00 (  0.79)\n",
      "17100\n",
      "[17100/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 5.5421e-01 (2.0677e+00)\tAcc@1   0.97 (  0.56)\tAcc@5   1.00 (  0.79)\n",
      "17200\n",
      "[17200/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.7599e+00 (2.0617e+00)\tAcc@1   0.67 (  0.56)\tAcc@5   0.91 (  0.79)\n",
      "17300\n",
      "[17300/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.8384e+00 (2.0547e+00)\tAcc@1   0.66 (  0.56)\tAcc@5   0.84 (  0.79)\n",
      "17400\n",
      "[17400/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.5650e-01 (2.0482e+00)\tAcc@1   0.99 (  0.56)\tAcc@5   1.00 (  0.79)\n",
      "17500\n",
      "[17500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.1477e+00 (2.0423e+00)\tAcc@1   0.94 (  0.57)\tAcc@5   1.00 (  0.80)\n",
      "Test: [ 0/79]\tTime  0.237 ( 0.237)\tLoss 1.8704e+00 (1.8704e+00)\tAcc@1   0.59 (  0.59)\tAcc@5   0.80 (  0.80)\n",
      " *   Acc@1 0.534 Acc@5 0.786\n",
      "17600\n",
      "[17600/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.4884e-01 (2.0369e+00)\tAcc@1   0.95 (  0.57)\tAcc@5   1.00 (  0.80)\n",
      "17700\n",
      "[17700/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.8428e+00 (2.0308e+00)\tAcc@1   0.55 (  0.57)\tAcc@5   0.86 (  0.80)\n",
      "17800\n",
      "[17800/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 8.0488e-01 (2.0234e+00)\tAcc@1   0.98 (  0.57)\tAcc@5   1.00 (  0.80)\n",
      "17900\n",
      "[17900/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.9879e-01 (2.0177e+00)\tAcc@1   0.99 (  0.57)\tAcc@5   1.00 (  0.80)\n",
      "18000\n",
      "[18000/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.6328e+00 (2.0112e+00)\tAcc@1   0.66 (  0.58)\tAcc@5   0.91 (  0.80)\n",
      "18100\n",
      "[18100/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 8.9314e-02 (2.0054e+00)\tAcc@1   1.00 (  0.58)\tAcc@5   1.00 (  0.80)\n",
      "18200\n",
      "[18200/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 4.5042e-01 (1.9990e+00)\tAcc@1   0.96 (  0.58)\tAcc@5   1.00 (  0.80)\n",
      "18300\n",
      "[18300/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.4513e-01 (1.9933e+00)\tAcc@1   0.98 (  0.58)\tAcc@5   1.00 (  0.80)\n",
      "18400\n",
      "[18400/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.0339e+00 (1.9879e+00)\tAcc@1   0.97 (  0.58)\tAcc@5   1.00 (  0.80)\n",
      "18500\n",
      "[18500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.0896e-01 (1.9818e+00)\tAcc@1   0.98 (  0.58)\tAcc@5   1.00 (  0.80)\n",
      "18600\n",
      "[18600/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.5264e+00 (1.9755e+00)\tAcc@1   0.67 (  0.59)\tAcc@5   0.94 (  0.81)\n",
      "18700\n",
      "[18700/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.7885e-01 (1.9706e+00)\tAcc@1   0.98 (  0.59)\tAcc@5   1.00 (  0.81)\n",
      "18800\n",
      "[18800/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.6693e+00 (1.9646e+00)\tAcc@1   0.97 (  0.59)\tAcc@5   1.00 (  0.81)\n",
      "18900\n",
      "[18900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.3500e+00 (1.9583e+00)\tAcc@1   0.98 (  0.59)\tAcc@5   1.00 (  0.81)\n",
      "19000\n",
      "[19000/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 6.2049e-01 (1.9527e+00)\tAcc@1   0.97 (  0.59)\tAcc@5   1.00 (  0.81)\n",
      "19100\n",
      "[19100/35156]\tTime  0.206 ( 0.206)\tData  0.000 ( 0.004)\tLoss 5.7974e-01 (1.9471e+00)\tAcc@1   0.98 (  0.59)\tAcc@5   1.00 (  0.81)\n",
      "19200\n",
      "[19200/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.0386e+00 (1.9417e+00)\tAcc@1   0.41 (  0.60)\tAcc@5   0.76 (  0.81)\n",
      "19300\n",
      "[19300/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.1624e-01 (1.9362e+00)\tAcc@1   0.98 (  0.60)\tAcc@5   1.00 (  0.81)\n",
      "19400\n",
      "[19400/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 7.8386e-02 (1.9303e+00)\tAcc@1   0.98 (  0.60)\tAcc@5   1.00 (  0.81)\n",
      "19500\n",
      "[19500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.4211e-01 (1.9257e+00)\tAcc@1   0.98 (  0.60)\tAcc@5   1.00 (  0.81)\n",
      "19600\n",
      "[19600/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.8150e+00 (1.9193e+00)\tAcc@1   0.63 (  0.60)\tAcc@5   0.84 (  0.81)\n",
      "19700\n",
      "[19700/35156]\tTime  0.209 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.3972e-01 (1.9142e+00)\tAcc@1   0.99 (  0.60)\tAcc@5   1.00 (  0.81)\n",
      "19800\n",
      "[19800/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.5158e-01 (1.9100e+00)\tAcc@1   0.98 (  0.61)\tAcc@5   1.00 (  0.82)\n",
      "19900\n",
      "[19900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 9.9209e-01 (1.9048e+00)\tAcc@1   0.97 (  0.61)\tAcc@5   1.00 (  0.82)\n",
      "20000\n",
      "[20000/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.5380e+00 (1.8996e+00)\tAcc@1   0.75 (  0.61)\tAcc@5   0.91 (  0.82)\n",
      "Test: [ 0/79]\tTime  0.234 ( 0.234)\tLoss 1.6868e+00 (1.6868e+00)\tAcc@1   0.65 (  0.65)\tAcc@5   0.80 (  0.80)\n",
      " *   Acc@1 0.550 Acc@5 0.788\n",
      "20100\n",
      "[20100/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.2542e+00 (1.8948e+00)\tAcc@1   0.30 (  0.61)\tAcc@5   0.62 (  0.82)\n",
      "20200\n",
      "[20200/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.2103e-01 (1.8889e+00)\tAcc@1   0.99 (  0.61)\tAcc@5   1.00 (  0.82)\n",
      "20300\n",
      "[20300/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.0272e+00 (1.8833e+00)\tAcc@1   0.96 (  0.61)\tAcc@5   1.00 (  0.82)\n",
      "20400\n",
      "[20400/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 7.7428e-01 (1.8783e+00)\tAcc@1   0.99 (  0.61)\tAcc@5   1.00 (  0.82)\n",
      "20500\n",
      "[20500/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 6.2096e-01 (1.8724e+00)\tAcc@1   1.00 (  0.62)\tAcc@5   1.00 (  0.82)\n",
      "20600\n",
      "[20600/35156]\tTime  0.196 ( 0.207)\tData  0.000 ( 0.004)\tLoss 7.6251e-01 (1.8676e+00)\tAcc@1   0.98 (  0.62)\tAcc@5   1.00 (  0.82)\n",
      "20700\n",
      "[20700/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.1989e+00 (1.8628e+00)\tAcc@1   0.38 (  0.62)\tAcc@5   0.65 (  0.82)\n",
      "20800\n",
      "[20800/35156]\tTime  0.202 ( 0.207)\tData  0.000 ( 0.004)\tLoss 6.9124e-02 (1.8589e+00)\tAcc@1   1.00 (  0.62)\tAcc@5   1.00 (  0.82)\n",
      "20900\n",
      "[20900/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 8.9668e-02 (1.8533e+00)\tAcc@1   0.99 (  0.62)\tAcc@5   1.00 (  0.82)\n",
      "21000\n",
      "[21000/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 5.5690e-02 (1.8483e+00)\tAcc@1   0.99 (  0.62)\tAcc@5   1.00 (  0.82)\n",
      "21100\n",
      "[21100/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 4.6356e-02 (1.8435e+00)\tAcc@1   1.00 (  0.62)\tAcc@5   1.00 (  0.82)\n",
      "21200\n",
      "[21200/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 6.2441e-02 (1.8391e+00)\tAcc@1   1.00 (  0.63)\tAcc@5   1.00 (  0.83)\n",
      "21300\n",
      "[21300/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.3469e-01 (1.8353e+00)\tAcc@1   0.98 (  0.63)\tAcc@5   1.00 (  0.83)\n",
      "21400\n",
      "[21400/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 5.8840e-01 (1.8309e+00)\tAcc@1   0.98 (  0.63)\tAcc@5   1.00 (  0.83)\n",
      "21500\n",
      "[21500/35156]\tTime  0.201 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.0935e-01 (1.8267e+00)\tAcc@1   1.00 (  0.63)\tAcc@5   1.00 (  0.83)\n",
      "21600\n",
      "[21600/35156]\tTime  0.202 ( 0.206)\tData  0.000 ( 0.004)\tLoss 7.3710e-02 (1.8217e+00)\tAcc@1   1.00 (  0.63)\tAcc@5   1.00 (  0.83)\n",
      "21700\n",
      "[21700/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.0740e-01 (1.8171e+00)\tAcc@1   0.99 (  0.63)\tAcc@5   1.00 (  0.83)\n",
      "21800\n",
      "[21800/35156]\tTime  0.206 ( 0.206)\tData  0.000 ( 0.004)\tLoss 6.4002e-02 (1.8123e+00)\tAcc@1   1.00 (  0.63)\tAcc@5   1.00 (  0.83)\n",
      "21900\n",
      "[21900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 8.5586e-01 (1.8088e+00)\tAcc@1   0.99 (  0.64)\tAcc@5   1.00 (  0.83)\n",
      "22000\n",
      "[22000/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.2799e-01 (1.8052e+00)\tAcc@1   1.00 (  0.64)\tAcc@5   1.00 (  0.83)\n",
      "22100\n",
      "[22100/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 8.8851e-02 (1.8007e+00)\tAcc@1   0.99 (  0.64)\tAcc@5   1.00 (  0.83)\n",
      "22200\n",
      "[22200/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.2018e-01 (1.7958e+00)\tAcc@1   0.99 (  0.64)\tAcc@5   1.00 (  0.83)\n",
      "22300\n",
      "[22300/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.5418e-01 (1.7916e+00)\tAcc@1   0.99 (  0.64)\tAcc@5   1.00 (  0.83)\n",
      "22400\n",
      "[22400/35156]\tTime  0.209 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.4746e-01 (1.7871e+00)\tAcc@1   1.00 (  0.64)\tAcc@5   1.00 (  0.83)\n",
      "22500\n",
      "[22500/35156]\tTime  0.198 ( 0.206)\tData  0.000 ( 0.004)\tLoss 6.0274e-01 (1.7822e+00)\tAcc@1   1.00 (  0.64)\tAcc@5   1.00 (  0.83)\n",
      "Test: [ 0/79]\tTime  0.227 ( 0.227)\tLoss 1.6271e+00 (1.6271e+00)\tAcc@1   0.64 (  0.64)\tAcc@5   0.81 (  0.81)\n",
      " *   Acc@1 0.555 Acc@5 0.791\n",
      "22600\n",
      "[22600/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 6.5086e-02 (1.7775e+00)\tAcc@1   1.00 (  0.64)\tAcc@5   1.00 (  0.83)\n",
      "22700\n",
      "[22700/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.9238e-02 (1.7727e+00)\tAcc@1   1.00 (  0.65)\tAcc@5   1.00 (  0.83)\n",
      "22800\n",
      "[22800/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.4096e+00 (1.7680e+00)\tAcc@1   0.79 (  0.65)\tAcc@5   0.95 (  0.84)\n",
      "22900\n",
      "[22900/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 4.3384e-02 (1.7642e+00)\tAcc@1   1.00 (  0.65)\tAcc@5   1.00 (  0.84)\n",
      "23000\n",
      "[23000/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 4.1990e-01 (1.7595e+00)\tAcc@1   0.99 (  0.65)\tAcc@5   1.00 (  0.84)\n",
      "23100\n",
      "[23100/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.3242e-01 (1.7556e+00)\tAcc@1   1.00 (  0.65)\tAcc@5   1.00 (  0.84)\n",
      "23200\n",
      "[23200/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 5.8764e-01 (1.7511e+00)\tAcc@1   0.99 (  0.65)\tAcc@5   1.00 (  0.84)\n",
      "23300\n",
      "[23300/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.2268e-01 (1.7478e+00)\tAcc@1   0.99 (  0.65)\tAcc@5   1.00 (  0.84)\n",
      "23400\n",
      "[23400/35156]\tTime  0.212 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.9971e-01 (1.7435e+00)\tAcc@1   0.99 (  0.65)\tAcc@5   1.00 (  0.84)\n",
      "23500\n",
      "[23500/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.4130e+00 (1.7392e+00)\tAcc@1   0.98 (  0.66)\tAcc@5   1.00 (  0.84)\n",
      "23600\n",
      "[23600/35156]\tTime  0.201 ( 0.207)\tData  0.000 ( 0.004)\tLoss 5.7506e-01 (1.7360e+00)\tAcc@1   1.00 (  0.66)\tAcc@5   1.00 (  0.84)\n",
      "23700\n",
      "[23700/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 5.1479e-01 (1.7322e+00)\tAcc@1   1.00 (  0.66)\tAcc@5   1.00 (  0.84)\n",
      "23800\n",
      "[23800/35156]\tTime  0.203 ( 0.207)\tData  0.003 ( 0.004)\tLoss 2.2587e+00 (1.7281e+00)\tAcc@1   0.80 (  0.66)\tAcc@5   0.92 (  0.84)\n",
      "23900\n",
      "[23900/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.3735e-02 (1.7246e+00)\tAcc@1   1.00 (  0.66)\tAcc@5   1.00 (  0.84)\n",
      "24000\n",
      "[24000/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 7.7686e-01 (1.7209e+00)\tAcc@1   1.00 (  0.66)\tAcc@5   1.00 (  0.84)\n",
      "24100\n",
      "[24100/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.6795e-01 (1.7167e+00)\tAcc@1   1.00 (  0.66)\tAcc@5   1.00 (  0.84)\n",
      "24200\n",
      "[24200/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 6.3711e-02 (1.7121e+00)\tAcc@1   0.99 (  0.66)\tAcc@5   1.00 (  0.84)\n",
      "24300\n",
      "[24300/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 5.1775e-01 (1.7089e+00)\tAcc@1   1.00 (  0.66)\tAcc@5   1.00 (  0.84)\n",
      "24400\n",
      "[24400/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.0629e+00 (1.7045e+00)\tAcc@1   0.91 (  0.67)\tAcc@5   0.99 (  0.84)\n",
      "24500\n",
      "[24500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.1379e+00 (1.7014e+00)\tAcc@1   0.40 (  0.67)\tAcc@5   0.63 (  0.85)\n",
      "24600\n",
      "[24600/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 4.5362e-02 (1.6978e+00)\tAcc@1   1.00 (  0.67)\tAcc@5   1.00 (  0.85)\n",
      "24700\n",
      "[24700/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.6982e+00 (1.6949e+00)\tAcc@1   0.96 (  0.67)\tAcc@5   0.99 (  0.85)\n",
      "24800\n",
      "[24800/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.3209e+00 (1.6916e+00)\tAcc@1   0.99 (  0.67)\tAcc@5   1.00 (  0.85)\n",
      "24900\n",
      "[24900/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 5.8169e-02 (1.6885e+00)\tAcc@1   1.00 (  0.67)\tAcc@5   1.00 (  0.85)\n",
      "25000\n",
      "[25000/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.4532e-02 (1.6853e+00)\tAcc@1   1.00 (  0.67)\tAcc@5   1.00 (  0.85)\n",
      "Test: [ 0/79]\tTime  0.232 ( 0.232)\tLoss 1.5202e+00 (1.5202e+00)\tAcc@1   0.62 (  0.62)\tAcc@5   0.85 (  0.85)\n",
      " *   Acc@1 0.568 Acc@5 0.799\n",
      "25100\n",
      "[25100/35156]\tTime  0.205 ( 0.207)\tData  0.000 ( 0.005)\tLoss 1.4536e+00 (1.6815e+00)\tAcc@1   0.99 (  0.67)\tAcc@5   1.00 (  0.85)\n",
      "25200\n",
      "[25200/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.4065e+00 (1.6784e+00)\tAcc@1   0.82 (  0.67)\tAcc@5   0.93 (  0.85)\n",
      "25300\n",
      "[25300/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.7100e+00 (1.6750e+00)\tAcc@1   0.64 (  0.67)\tAcc@5   0.84 (  0.85)\n",
      "25400\n",
      "[25400/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.3229e-02 (1.6713e+00)\tAcc@1   1.00 (  0.68)\tAcc@5   1.00 (  0.85)\n",
      "25500\n",
      "[25500/35156]\tTime  0.202 ( 0.207)\tData  0.000 ( 0.004)\tLoss 8.5791e-01 (1.6671e+00)\tAcc@1   1.00 (  0.68)\tAcc@5   1.00 (  0.85)\n",
      "25600\n",
      "[25600/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 7.5530e-02 (1.6631e+00)\tAcc@1   1.00 (  0.68)\tAcc@5   1.00 (  0.85)\n",
      "25700\n",
      "[25700/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 4.6856e-02 (1.6598e+00)\tAcc@1   1.00 (  0.68)\tAcc@5   1.00 (  0.85)\n",
      "25800\n",
      "[25800/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 4.7961e-02 (1.6569e+00)\tAcc@1   1.00 (  0.68)\tAcc@5   1.00 (  0.85)\n",
      "25900\n",
      "[25900/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.5957e+00 (1.6532e+00)\tAcc@1   0.64 (  0.68)\tAcc@5   0.89 (  0.85)\n",
      "26000\n",
      "[26000/35156]\tTime  0.205 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.1899e-01 (1.6499e+00)\tAcc@1   0.99 (  0.68)\tAcc@5   1.00 (  0.85)\n",
      "26100\n",
      "[26100/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.8363e-02 (1.6470e+00)\tAcc@1   1.00 (  0.68)\tAcc@5   1.00 (  0.85)\n",
      "26200\n",
      "[26200/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.8531e-02 (1.6432e+00)\tAcc@1   1.00 (  0.68)\tAcc@5   1.00 (  0.85)\n",
      "26300\n",
      "[26300/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 4.1469e-02 (1.6396e+00)\tAcc@1   1.00 (  0.68)\tAcc@5   1.00 (  0.85)\n",
      "26400\n",
      "[26400/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.1748e-02 (1.6357e+00)\tAcc@1   1.00 (  0.69)\tAcc@5   1.00 (  0.85)\n",
      "26500\n",
      "[26500/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.7287e+00 (1.6324e+00)\tAcc@1   0.59 (  0.69)\tAcc@5   0.85 (  0.85)\n",
      "26600\n",
      "[26600/35156]\tTime  0.208 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.8385e+00 (1.6287e+00)\tAcc@1   0.46 (  0.69)\tAcc@5   0.73 (  0.86)\n",
      "26700\n",
      "[26700/35156]\tTime  0.196 ( 0.206)\tData  0.000 ( 0.004)\tLoss 7.6558e-01 (1.6250e+00)\tAcc@1   1.00 (  0.69)\tAcc@5   1.00 (  0.86)\n",
      "26800\n",
      "[26800/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 6.4136e-02 (1.6215e+00)\tAcc@1   1.00 (  0.69)\tAcc@5   1.00 (  0.86)\n",
      "26900\n",
      "[26900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.9936e+00 (1.6194e+00)\tAcc@1   0.88 (  0.69)\tAcc@5   0.98 (  0.86)\n",
      "27000\n",
      "[27000/35156]\tTime  0.196 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.6085e+00 (1.6155e+00)\tAcc@1   0.97 (  0.69)\tAcc@5   1.00 (  0.86)\n",
      "27100\n",
      "[27100/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 4.3950e-01 (1.6123e+00)\tAcc@1   1.00 (  0.69)\tAcc@5   1.00 (  0.86)\n",
      "27200\n",
      "[27200/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 8.4363e-01 (1.6091e+00)\tAcc@1   1.00 (  0.69)\tAcc@5   1.00 (  0.86)\n",
      "27300\n",
      "[27300/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 6.3999e-02 (1.6066e+00)\tAcc@1   1.00 (  0.69)\tAcc@5   1.00 (  0.86)\n",
      "27400\n",
      "[27400/35156]\tTime  0.205 ( 0.206)\tData  0.000 ( 0.004)\tLoss 6.0859e-02 (1.6031e+00)\tAcc@1   1.00 (  0.70)\tAcc@5   1.00 (  0.86)\n",
      "27500\n",
      "[27500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.1963e-01 (1.6002e+00)\tAcc@1   1.00 (  0.70)\tAcc@5   1.00 (  0.86)\n",
      "Test: [ 0/79]\tTime  0.227 ( 0.227)\tLoss 1.5536e+00 (1.5536e+00)\tAcc@1   0.64 (  0.64)\tAcc@5   0.82 (  0.82)\n",
      " *   Acc@1 0.576 Acc@5 0.807\n",
      "27600\n",
      "[27600/35156]\tTime  0.209 ( 0.207)\tData  0.000 ( 0.005)\tLoss 8.1525e-02 (1.5968e+00)\tAcc@1   1.00 (  0.70)\tAcc@5   1.00 (  0.86)\n",
      "27700\n",
      "[27700/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 4.9642e-02 (1.5940e+00)\tAcc@1   1.00 (  0.70)\tAcc@5   1.00 (  0.86)\n",
      "27800\n",
      "[27800/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.4940e-01 (1.5912e+00)\tAcc@1   1.00 (  0.70)\tAcc@5   1.00 (  0.86)\n",
      "27900\n",
      "[27900/35156]\tTime  0.205 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.0063e-02 (1.5880e+00)\tAcc@1   1.00 (  0.70)\tAcc@5   1.00 (  0.86)\n",
      "28000\n",
      "[28000/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.5481e-01 (1.5844e+00)\tAcc@1   1.00 (  0.70)\tAcc@5   1.00 (  0.86)\n",
      "28100\n",
      "[28100/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 8.9500e-01 (1.5810e+00)\tAcc@1   1.00 (  0.70)\tAcc@5   1.00 (  0.86)\n",
      "28200\n",
      "[28200/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.7133e-01 (1.5776e+00)\tAcc@1   1.00 (  0.70)\tAcc@5   1.00 (  0.86)\n",
      "28300\n",
      "[28300/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.4729e+00 (1.5747e+00)\tAcc@1   1.00 (  0.70)\tAcc@5   1.00 (  0.86)\n",
      "28400\n",
      "[28400/35156]\tTime  0.206 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.0234e+00 (1.5719e+00)\tAcc@1   1.00 (  0.70)\tAcc@5   1.00 (  0.86)\n",
      "28500\n",
      "[28500/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.1804e-02 (1.5683e+00)\tAcc@1   1.00 (  0.70)\tAcc@5   1.00 (  0.86)\n",
      "28600\n",
      "[28600/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.6128e+00 (1.5654e+00)\tAcc@1   0.98 (  0.71)\tAcc@5   1.00 (  0.86)\n",
      "28700\n",
      "[28700/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.6623e-02 (1.5628e+00)\tAcc@1   1.00 (  0.71)\tAcc@5   1.00 (  0.86)\n",
      "28800\n",
      "[28800/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.3516e-02 (1.5600e+00)\tAcc@1   1.00 (  0.71)\tAcc@5   1.00 (  0.86)\n",
      "28900\n",
      "[28900/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.0676e-02 (1.5578e+00)\tAcc@1   1.00 (  0.71)\tAcc@5   1.00 (  0.86)\n",
      "29000\n",
      "[29000/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.7011e+00 (1.5548e+00)\tAcc@1   0.96 (  0.71)\tAcc@5   0.99 (  0.87)\n",
      "29100\n",
      "[29100/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 6.8371e-01 (1.5523e+00)\tAcc@1   0.99 (  0.71)\tAcc@5   1.00 (  0.87)\n",
      "29200\n",
      "[29200/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.6976e+00 (1.5494e+00)\tAcc@1   0.98 (  0.71)\tAcc@5   1.00 (  0.87)\n",
      "29300\n",
      "[29300/35156]\tTime  0.205 ( 0.206)\tData  0.000 ( 0.004)\tLoss 9.4635e-01 (1.5468e+00)\tAcc@1   1.00 (  0.71)\tAcc@5   1.00 (  0.87)\n",
      "29400\n",
      "[29400/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.4139e+00 (1.5441e+00)\tAcc@1   0.99 (  0.71)\tAcc@5   1.00 (  0.87)\n",
      "29500\n",
      "[29500/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.5127e-02 (1.5412e+00)\tAcc@1   1.00 (  0.71)\tAcc@5   1.00 (  0.87)\n",
      "29600\n",
      "[29600/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.6439e+00 (1.5390e+00)\tAcc@1   0.97 (  0.71)\tAcc@5   1.00 (  0.87)\n",
      "29700\n",
      "[29700/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 4.3020e-02 (1.5362e+00)\tAcc@1   1.00 (  0.71)\tAcc@5   1.00 (  0.87)\n",
      "29800\n",
      "[29800/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 4.1065e-01 (1.5335e+00)\tAcc@1   1.00 (  0.72)\tAcc@5   1.00 (  0.87)\n",
      "29900\n",
      "[29900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 4.1295e-02 (1.5306e+00)\tAcc@1   1.00 (  0.72)\tAcc@5   1.00 (  0.87)\n",
      "30000\n",
      "[30000/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 5.5851e-02 (1.5276e+00)\tAcc@1   1.00 (  0.72)\tAcc@5   1.00 (  0.87)\n",
      "Test: [ 0/79]\tTime  0.233 ( 0.233)\tLoss 1.4723e+00 (1.4723e+00)\tAcc@1   0.65 (  0.65)\tAcc@5   0.85 (  0.85)\n",
      " *   Acc@1 0.594 Acc@5 0.812\n",
      "30100\n",
      "[30100/35156]\tTime  0.205 ( 0.207)\tData  0.000 ( 0.005)\tLoss 2.7584e-02 (1.5253e+00)\tAcc@1   1.00 (  0.72)\tAcc@5   1.00 (  0.87)\n",
      "30200\n",
      "[30200/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.2267e-01 (1.5231e+00)\tAcc@1   1.00 (  0.72)\tAcc@5   1.00 (  0.87)\n",
      "30300\n",
      "[30300/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.2501e+00 (1.5205e+00)\tAcc@1   0.76 (  0.72)\tAcc@5   0.95 (  0.87)\n",
      "30400\n",
      "[30400/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.0283e-02 (1.5179e+00)\tAcc@1   1.00 (  0.72)\tAcc@5   1.00 (  0.87)\n",
      "30500\n",
      "[30500/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.4846e-01 (1.5151e+00)\tAcc@1   1.00 (  0.72)\tAcc@5   1.00 (  0.87)\n",
      "30600\n",
      "[30600/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.0042e-01 (1.5124e+00)\tAcc@1   1.00 (  0.72)\tAcc@5   1.00 (  0.87)\n",
      "30700\n",
      "[30700/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.2821e+00 (1.5099e+00)\tAcc@1   1.00 (  0.72)\tAcc@5   1.00 (  0.87)\n",
      "30800\n",
      "[30800/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.2456e-01 (1.5077e+00)\tAcc@1   1.00 (  0.72)\tAcc@5   1.00 (  0.87)\n",
      "30900\n",
      "[30900/35156]\tTime  0.213 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.0595e-02 (1.5047e+00)\tAcc@1   1.00 (  0.72)\tAcc@5   1.00 (  0.87)\n",
      "31000\n",
      "[31000/35156]\tTime  0.197 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.5804e+00 (1.5024e+00)\tAcc@1   0.55 (  0.72)\tAcc@5   0.77 (  0.87)\n",
      "31100\n",
      "[31100/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.5248e-01 (1.4999e+00)\tAcc@1   1.00 (  0.73)\tAcc@5   1.00 (  0.87)\n",
      "31200\n",
      "[31200/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.0726e-01 (1.4971e+00)\tAcc@1   1.00 (  0.73)\tAcc@5   1.00 (  0.87)\n",
      "31300\n",
      "[31300/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.0961e-01 (1.4947e+00)\tAcc@1   1.00 (  0.73)\tAcc@5   1.00 (  0.87)\n",
      "31400\n",
      "[31400/35156]\tTime  0.211 ( 0.207)\tData  0.000 ( 0.004)\tLoss 4.0845e-01 (1.4922e+00)\tAcc@1   1.00 (  0.73)\tAcc@5   1.00 (  0.87)\n",
      "31500\n",
      "[31500/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.5406e-02 (1.4893e+00)\tAcc@1   1.00 (  0.73)\tAcc@5   1.00 (  0.87)\n",
      "31600\n",
      "[31600/35156]\tTime  0.202 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.7500e+00 (1.4870e+00)\tAcc@1   0.95 (  0.73)\tAcc@5   0.99 (  0.87)\n",
      "31700\n",
      "[31700/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 5.6575e-01 (1.4846e+00)\tAcc@1   1.00 (  0.73)\tAcc@5   1.00 (  0.88)\n",
      "31800\n",
      "[31800/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.4424e+00 (1.4821e+00)\tAcc@1   1.00 (  0.73)\tAcc@5   1.00 (  0.88)\n",
      "31900\n",
      "[31900/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.1569e-01 (1.4798e+00)\tAcc@1   1.00 (  0.73)\tAcc@5   1.00 (  0.88)\n",
      "32000\n",
      "[32000/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.4742e-01 (1.4771e+00)\tAcc@1   1.00 (  0.73)\tAcc@5   1.00 (  0.88)\n",
      "32100\n",
      "[32100/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 9.3422e-01 (1.4745e+00)\tAcc@1   1.00 (  0.73)\tAcc@5   1.00 (  0.88)\n",
      "32200\n",
      "[32200/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.0289e+00 (1.4722e+00)\tAcc@1   0.89 (  0.73)\tAcc@5   0.99 (  0.88)\n",
      "32300\n",
      "[32300/35156]\tTime  0.205 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.7559e-01 (1.4697e+00)\tAcc@1   1.00 (  0.73)\tAcc@5   1.00 (  0.88)\n",
      "32400\n",
      "[32400/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 6.1728e-01 (1.4673e+00)\tAcc@1   1.00 (  0.73)\tAcc@5   1.00 (  0.88)\n",
      "32500\n",
      "[32500/35156]\tTime  0.205 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.7459e-02 (1.4649e+00)\tAcc@1   1.00 (  0.73)\tAcc@5   1.00 (  0.88)\n",
      "Test: [ 0/79]\tTime  0.225 ( 0.225)\tLoss 1.4664e+00 (1.4664e+00)\tAcc@1   0.66 (  0.66)\tAcc@5   0.85 (  0.85)\n",
      " *   Acc@1 0.598 Acc@5 0.814\n",
      "32600\n",
      "[32600/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.005)\tLoss 4.8279e-01 (1.4626e+00)\tAcc@1   1.00 (  0.74)\tAcc@5   1.00 (  0.88)\n",
      "32700\n",
      "[32700/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.5285e-01 (1.4604e+00)\tAcc@1   1.00 (  0.74)\tAcc@5   1.00 (  0.88)\n",
      "32800\n",
      "[32800/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.0737e+00 (1.4585e+00)\tAcc@1   0.89 (  0.74)\tAcc@5   0.98 (  0.88)\n",
      "32900\n",
      "[32900/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.1164e-02 (1.4560e+00)\tAcc@1   1.00 (  0.74)\tAcc@5   1.00 (  0.88)\n",
      "33000\n",
      "[33000/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.6444e+00 (1.4543e+00)\tAcc@1   0.47 (  0.74)\tAcc@5   0.79 (  0.88)\n",
      "33100\n",
      "[33100/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.6249e+00 (1.4523e+00)\tAcc@1   0.48 (  0.74)\tAcc@5   0.84 (  0.88)\n",
      "33200\n",
      "[33200/35156]\tTime  0.205 ( 0.207)\tData  0.000 ( 0.004)\tLoss 6.8391e-01 (1.4504e+00)\tAcc@1   1.00 (  0.74)\tAcc@5   1.00 (  0.88)\n",
      "33300\n",
      "[33300/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.6409e+00 (1.4476e+00)\tAcc@1   0.46 (  0.74)\tAcc@5   0.83 (  0.88)\n",
      "33400\n",
      "[33400/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.8964e+00 (1.4459e+00)\tAcc@1   0.96 (  0.74)\tAcc@5   0.99 (  0.88)\n",
      "33500\n",
      "[33500/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.4413e-02 (1.4438e+00)\tAcc@1   1.00 (  0.74)\tAcc@5   1.00 (  0.88)\n",
      "33600\n",
      "[33600/35156]\tTime  0.201 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.2122e+00 (1.4417e+00)\tAcc@1   1.00 (  0.74)\tAcc@5   1.00 (  0.88)\n",
      "33700\n",
      "[33700/35156]\tTime  0.205 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.4316e-02 (1.4391e+00)\tAcc@1   1.00 (  0.74)\tAcc@5   1.00 (  0.88)\n",
      "33800\n",
      "[33800/35156]\tTime  0.205 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.2471e-01 (1.4367e+00)\tAcc@1   1.00 (  0.74)\tAcc@5   1.00 (  0.88)\n",
      "33900\n",
      "[33900/35156]\tTime  0.203 ( 0.207)\tData  0.000 ( 0.004)\tLoss 1.0527e-01 (1.4343e+00)\tAcc@1   0.99 (  0.74)\tAcc@5   1.00 (  0.88)\n",
      "34000\n",
      "[34000/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 3.0265e-01 (1.4321e+00)\tAcc@1   1.00 (  0.74)\tAcc@5   1.00 (  0.88)\n",
      "34100\n",
      "[34100/35156]\tTime  0.202 ( 0.207)\tData  0.000 ( 0.004)\tLoss 2.6391e-02 (1.4298e+00)\tAcc@1   1.00 (  0.75)\tAcc@5   1.00 (  0.88)\n",
      "34200\n",
      "[34200/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.2394e-01 (1.4277e+00)\tAcc@1   1.00 (  0.75)\tAcc@5   1.00 (  0.88)\n",
      "34300\n",
      "[34300/35156]\tTime  0.198 ( 0.206)\tData  0.000 ( 0.004)\tLoss 2.5063e+00 (1.4258e+00)\tAcc@1   0.62 (  0.75)\tAcc@5   0.88 (  0.88)\n",
      "34400\n",
      "[34400/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.1609e+00 (1.4235e+00)\tAcc@1   1.00 (  0.75)\tAcc@5   1.00 (  0.88)\n",
      "34500\n",
      "[34500/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.7615e+00 (1.4212e+00)\tAcc@1   0.98 (  0.75)\tAcc@5   1.00 (  0.88)\n",
      "34600\n",
      "[34600/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.4680e-01 (1.4191e+00)\tAcc@1   1.00 (  0.75)\tAcc@5   1.00 (  0.88)\n",
      "34700\n",
      "[34700/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.0746e-01 (1.4173e+00)\tAcc@1   1.00 (  0.75)\tAcc@5   1.00 (  0.88)\n",
      "34800\n",
      "[34800/35156]\tTime  0.204 ( 0.206)\tData  0.000 ( 0.004)\tLoss 6.5517e-02 (1.4153e+00)\tAcc@1   1.00 (  0.75)\tAcc@5   1.00 (  0.88)\n",
      "34900\n",
      "[34900/35156]\tTime  0.205 ( 0.206)\tData  0.000 ( 0.004)\tLoss 3.0199e-02 (1.4132e+00)\tAcc@1   1.00 (  0.75)\tAcc@5   1.00 (  0.89)\n",
      "35000\n",
      "[35000/35156]\tTime  0.203 ( 0.206)\tData  0.000 ( 0.004)\tLoss 1.3196e+00 (1.4112e+00)\tAcc@1   0.99 (  0.75)\tAcc@5   1.00 (  0.89)\n",
      "Test: [ 0/79]\tTime  0.231 ( 0.231)\tLoss 1.4109e+00 (1.4109e+00)\tAcc@1   0.66 (  0.66)\tAcc@5   0.84 (  0.84)\n",
      " *   Acc@1 0.600 Acc@5 0.816\n",
      "35100\n",
      "[35100/35156]\tTime  0.204 ( 0.207)\tData  0.000 ( 0.004)\tLoss 6.6228e-02 (1.4089e+00)\tAcc@1   1.00 (  0.75)\tAcc@5   1.00 (  0.89)\n",
      "Test: [ 0/79]\tTime  0.221 ( 0.221)\tLoss 1.4109e+00 (1.4109e+00)\tAcc@1   0.66 (  0.66)\tAcc@5   0.84 (  0.84)\n",
      " *   Acc@1 0.600 Acc@5 0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 batch_time â–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–„â–ƒâ–„â–„â–â–ƒâ–ƒâ–ˆâ–„â–ƒâ–„â–„â–„â–„â–„â–ƒâ–„â–ƒâ–„â–‡â–„â–ƒâ–ƒâ–„â–„â–‡â–…â–„â–„â–ƒâ–„â–ƒâ–‚â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  data_time â–‡â–ƒâ–ƒâ–‡â–†â–‡â–„â–†â–†â–†â–‡â–†â–†â–†â–†â–‡â–ƒâ–…â–„â–„â–„â–†â–‡â–†â–ƒâ–†â–ƒâ–ˆâ–ƒâ–…â–…â–†â–ƒâ–‡â–†â–â–ƒâ–†â–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   l2_grads â–†â–‡â–ˆâ–ˆâ–ˆâ–‡â–†â–‡â–†â–†â–‡â–†â–…â–…â–…â–…â–…â–…â–ƒâ–‚â–„â–ƒâ–…â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–â–â–‚â–ƒâ–â–â–â–„â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  l2_params â–â–â–â–â–â–â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         lr â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         samples_per_second â–†â–†â–…â–…â–…â–†â–…â–…â–…â–…â–ˆâ–†â–†â–â–…â–†â–…â–…â–…â–…â–…â–†â–…â–†â–…â–‚â–…â–†â–…â–…â–…â–‚â–„â–…â–…â–†â–…â–†â–‡â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: samples_per_second_per_gpu â–†â–†â–…â–…â–…â–†â–…â–…â–…â–…â–ˆâ–†â–†â–â–…â–†â–…â–…â–…â–…â–…â–†â–…â–†â–…â–‚â–…â–†â–…â–…â–…â–‚â–„â–…â–…â–†â–…â–†â–‡â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/acc@1 â–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–†â–…â–†â–†â–†â–†â–…â–‡â–†â–ˆâ–ˆâ–…â–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/acc@5 â–â–„â–…â–…â–…â–„â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–†â–†â–†â–…â–‡â–†â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–†â–‚â–…â–ƒâ–â–…â–„â–†â–‚â–â–â–‚â–‚â–ƒâ–â–â–â–ƒâ–„â–â–â–â–„â–ƒâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 batch_time 0.20396\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  data_time 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   l2_grads 0.17104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  l2_params 262.22915\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         samples_per_second 627.58302\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: samples_per_second_per_gpu 627.58302\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/acc@1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/acc@5 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 0.06623\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mBaseline_ImageNet_run\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atharv_m-iit-roorkee/fractual_transformer/runs/sfgxihe6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/atharv_m-iit-roorkee/fractual_transformer\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20241122_213057-sfgxihe6/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
     ]
    }
   ],
   "source": [
    "### Necessary Imports and dependencies\n",
    "### Wandb project_name is baseline_ImageNet\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms as transforms\n",
    "from typing import Any, Dict, Union, Type, Callable, Optional, List\n",
    "from torchvision.models.vision_transformer import MLPBlock\n",
    "import wandb\n",
    "\n",
    "\n",
    "num_epochs=90\n",
    "\n",
    "# Parameters specific to CIFAR-10\n",
    "batch_size = 128\n",
    "num_workers = 4 \n",
    "\n",
    "# Dataset loading code\n",
    "# Define CIFAR-10 datasets\n",
    "train_dataset = datasets.CIFAR100(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
    "    ])\n",
    ")\n",
    "\n",
    "val_dataset = datasets.CIFAR100(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261]),\n",
    "    ])\n",
    ")\n",
    "\n",
    "n = len(train_dataset)\n",
    "\n",
    "total_steps = round((n * num_epochs) / batch_size)\n",
    "\n",
    "start_step=0\n",
    "\n",
    "mixup = v2.MixUp(alpha=0.2, num_classes=100)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=lambda batch: mixup(*torch.utils.data.default_collate(batch)), \n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "warmup_try=10000\n",
    "\n",
    "# Taken from https://github.com/lucidrains/vit-pytorch, likely ported from https://github.com/google-research/big_vision/\n",
    "def posemb_sincos_2d(h, w, dim, temperature: int = 10000, dtype = torch.float32):\n",
    "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n",
    "    assert (dim % 4) == 0, \"feature dimension must be multiple of 4 for sincos emb\"\n",
    "    omega = torch.arange(dim // 4) / (dim // 4 - 1)\n",
    "    omega = 1.0 / (temperature ** omega)\n",
    "\n",
    "    y = y.flatten()[:, None] * omega[None, :]\n",
    "    x = x.flatten()[:, None] * omega[None, :]\n",
    "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)\n",
    "    return pe.type(dtype)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Attention block\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP block\n",
    "        self.ln_2 = norm_layer(hidden_dim)\n",
    "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
    "\n",
    "        # Fix init discrepancy between nn.MultiheadAttention and that of big_vision\n",
    "        bound = math.sqrt(3 / hidden_dim)\n",
    "        nn.init.uniform_(self.self_attention.in_proj_weight, -bound, bound)\n",
    "        nn.init.uniform_(self.self_attention.out_proj.weight, -bound, bound)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        x = self.ln_1(input)\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
    "        x = self.dropout(x)\n",
    "        x = x + input\n",
    "\n",
    "        y = self.ln_2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        for i in range(num_layers):\n",
    "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
    "                num_heads,\n",
    "                hidden_dim,\n",
    "                mlp_dim,\n",
    "                dropout,\n",
    "                attention_dropout,\n",
    "                norm_layer,\n",
    "            )\n",
    "        self.layers = nn.Sequential(layers)\n",
    "        self.ln = norm_layer(hidden_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        return self.ln(self.layers(self.dropout(input)))\n",
    "\n",
    "\n",
    "class SimpleVisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer modified per https://arxiv.org/abs/2205.01580.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int,\n",
    "        patch_size: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float = 0.0,\n",
    "        attention_dropout: float = 0.0,\n",
    "        num_classes: int = 100,\n",
    "        representation_size: Optional[int] = None,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        torch._assert(image_size % patch_size == 0, \"Input shape indivisible by patch size!\")\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        self.representation_size = representation_size\n",
    "        self.norm_layer = norm_layer\n",
    "\n",
    "        self.conv_proj = nn.Conv2d(\n",
    "            in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "        h = w = image_size // patch_size\n",
    "        seq_length = h * w\n",
    "        self.register_buffer(\"pos_embedding\", posemb_sincos_2d(h=h, w=w, dim=hidden_dim))\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            seq_length,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            hidden_dim,\n",
    "            mlp_dim,\n",
    "            dropout,\n",
    "            attention_dropout,\n",
    "            norm_layer,\n",
    "        )\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        if representation_size is None:\n",
    "            heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n",
    "        else:\n",
    "            heads_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n",
    "            heads_layers[\"act\"] = nn.Tanh()\n",
    "            heads_layers[\"head\"] = nn.Linear(representation_size, num_classes)\n",
    "\n",
    "        self.heads = nn.Sequential(heads_layers)\n",
    "\n",
    "        if isinstance(self.conv_proj, nn.Conv2d):\n",
    "            # Init the patchify stem\n",
    "            fan_in = self.conv_proj.in_channels * self.conv_proj.kernel_size[0] * self.conv_proj.kernel_size[1]\n",
    "            # constant is stddev of standard normal truncated to (-2, 2)\n",
    "            std = math.sqrt(1 / fan_in) / .87962566103423978\n",
    "            nn.init.trunc_normal_(self.conv_proj.weight, std=std, a=-2 * std, b=2 * std)\n",
    "            if self.conv_proj.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.bias)\n",
    "        elif self.conv_proj.conv_last is not None and isinstance(self.conv_proj.conv_last, nn.Conv2d):\n",
    "            # Init the last 1x1 conv of the conv stem\n",
    "            nn.init.normal_(\n",
    "                self.conv_proj.conv_last.weight, mean=0.0, std=math.sqrt(2.0 / self.conv_proj.conv_last.out_channels)\n",
    "            )\n",
    "            if self.conv_proj.conv_last.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.conv_last.bias)\n",
    "\n",
    "        if hasattr(self.heads, \"pre_logits\") and isinstance(self.heads.pre_logits, nn.Linear):\n",
    "            fan_in = self.heads.pre_logits.in_features\n",
    "            nn.init.trunc_normal_(self.heads.pre_logits.weight, std=math.sqrt(1 / fan_in))\n",
    "            nn.init.zeros_(self.heads.pre_logits.bias)\n",
    "\n",
    "        if isinstance(self.heads.head, nn.Linear):\n",
    "            nn.init.zeros_(self.heads.head.weight)\n",
    "            nn.init.zeros_(self.heads.head.bias)\n",
    "\n",
    "    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        n, c, h, w = x.shape\n",
    "        p = self.patch_size\n",
    "        torch._assert(h == self.image_size, f\"Wrong image height! Expected {self.image_size} but got {h}!\")\n",
    "        torch._assert(w == self.image_size, f\"Wrong image width! Expected {self.image_size} but got {w}!\")\n",
    "        n_h = h // p\n",
    "        n_w = w // p\n",
    "\n",
    "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n",
    "        x = self.conv_proj(x)\n",
    "        \n",
    "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n",
    "        x = x.reshape(n, self.hidden_dim, n_h * n_w)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n",
    "        # The self attention layer expects inputs in the format (N, S, E)\n",
    "        # where S is the source sequence length, N is the batch size, E is the\n",
    "        # embedding dimension\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Reshape and permute the input tensor\n",
    "        x = self._process_input(x)\n",
    "        x = x + self.pos_embedding\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim = 1)\n",
    "        x = self.heads(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def weight_decay_param(n, p):\n",
    "    return p.ndim >= 2 and n.endswith('weight')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create model\n",
    "model = SimpleVisionTransformer(\n",
    "    image_size=32,\n",
    "    patch_size=4,\n",
    "    num_layers=12,\n",
    "    num_heads=6,\n",
    "    hidden_dim=384,\n",
    "    mlp_dim=1536,\n",
    ").to(device)\n",
    "wd_params = [p for n, p in model.named_parameters() if weight_decay_param(n, p) and p.requires_grad]\n",
    "non_wd_params = [p for n, p in model.named_parameters() if not weight_decay_param(n, p) and p.requires_grad]\n",
    "\n",
    "original_model = model\n",
    "\n",
    "weight_decay = 0.1\n",
    "learning_rate = 1e-3\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": wd_params, \"weight_decay\": 0.1},\n",
    "        {\"params\": non_wd_params, \"weight_decay\": 0.},\n",
    "    ],\n",
    "    lr=learning_rate,\n",
    ")\n",
    "\n",
    "warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: step / warmup_try)\n",
    "cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps - warmup_try)\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, [warmup, cosine], [warmup_try])\n",
    "\n",
    "## Path_to_be_changed\n",
    "checkpoint_path = \"/kaggle/working/\"\n",
    "\n",
    "def save_checkpoint(state, is_best, path, filename='baselinecheckpoint_imagenet.pth.tar'):\n",
    "    filename = os.path.join(path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(path, 'model_best.pth.tar'))\n",
    "\n",
    "class Summary(Enum):\n",
    "    NONE = 0\n",
    "    AVERAGE = 1\n",
    "    SUM = 2\n",
    "    COUNT = 3\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f', summary_type=Summary.AVERAGE):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.summary_type = summary_type\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def all_reduce(self):\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        total = torch.tensor([self.sum, self.count], dtype=torch.float32, device=device)\n",
    "        dist.all_reduce(total, dist.ReduceOp.SUM, async_op=False)\n",
    "        self.sum, self.count = total.tolist()\n",
    "        self.avg = self.sum / self.count\n",
    "    \n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "    \n",
    "    def summary(self):\n",
    "        fmtstr = ''\n",
    "        if self.summary_type is Summary.NONE:\n",
    "            fmtstr = ''\n",
    "        elif self.summary_type is Summary.AVERAGE:\n",
    "            fmtstr = '{name} {avg:.3f}'\n",
    "        elif self.summary_type is Summary.SUM:\n",
    "            fmtstr = '{name} {sum:.3f}'\n",
    "        elif self.summary_type is Summary.COUNT:\n",
    "            fmtstr = '{name} {count:.3f}'\n",
    "        else:\n",
    "            raise ValueError('invalid summary type %r' % self.summary_type)\n",
    "        \n",
    "        return fmtstr.format(**self.__dict__)\n",
    "    \n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "        \n",
    "    def display_summary(self):\n",
    "        entries = [\" *\"]\n",
    "        entries += [meter.summary() for meter in self.meters]\n",
    "        print(' '.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "    \n",
    "def accuracy(output, target, topk=(1,), class_prob=False):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        \n",
    "        # with e.g. MixUp target is now given by probabilities for each class so we need to convert to class indices\n",
    "        if class_prob:\n",
    "            _, target = target.topk(1, 1, True, True)\n",
    "            target = target.squeeze(dim=1)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(1.0 / batch_size))\n",
    "        return res\n",
    "    \n",
    "log_steps = 2500\n",
    "\n",
    "wandb.login(key=\"2d1b2da6b789a71e0c259cede4c9b770b2e44281\")\n",
    "\n",
    "# Initialize a new run\n",
    "wandb.init(project=\"fractual_transformer\", name=\"Baseline_ImageNet_run\")\n",
    "\n",
    "def validate(val_loader, model, criterion, step, use_wandb=False, accum_freq=1, print_freq=100):\n",
    "    \n",
    "    def run_validate(loader, base_progress=0):\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "            end = time.time()\n",
    "            for i, (images, target) in enumerate(loader):\n",
    "                i = base_progress + i\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    images = images.cuda(non_blocking=True)\n",
    "                    target = target.cuda(non_blocking=True)\n",
    "                elif torch.backends.mps.is_available():\n",
    "                    images = images.to('mps')\n",
    "                    target = target.to('mps')\n",
    "\n",
    "                for img, trt in zip(images.chunk(accum_freq), target.chunk(accum_freq)):\n",
    "                    # compute output\n",
    "                    output = model(img)\n",
    "                    loss = criterion(output, trt)\n",
    "\n",
    "                    # measure accuracy and record loss\n",
    "                    acc1, acc5 = accuracy(output, trt, topk=(1, 5))\n",
    "                    losses.update(loss.item(), img.size(0))\n",
    "                    top1.update(acc1[0].item(), img.size(0))\n",
    "                    top5.update(acc5[0].item(), img.size(0))\n",
    "                    \n",
    "                # measure elapsed time\n",
    "                batch_time.update(time.time() - end)\n",
    "                end = time.time()\n",
    "\n",
    "                if i % print_freq == 0:\n",
    "                    progress.display(i)\n",
    "\n",
    "    batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n",
    "    losses = AverageMeter('Loss', ':.4e', Summary.NONE)\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    run_validate(val_loader)\n",
    "\n",
    "    progress.display_summary()\n",
    "\n",
    "    if use_wandb:        \n",
    "        log_data = {\n",
    "            'val/loss': losses.avg,\n",
    "            'val/acc@1': top1.avg,\n",
    "            'val/acc@5': top5.avg,\n",
    "        }\n",
    "        wandb.log(log_data, step=step)\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "def train(train_loader, val_loader, start_step, total_steps, original_model, model, criterion, optimizer, scheduler, device):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    print_freq = 100  # Print frequency (adjust as needed)\n",
    "    log_steps = 2500  # Log steps (adjust as needed)\n",
    "    accum_freq = 1  # Gradient accumulation frequency (adjust as needed)\n",
    "    \n",
    "    progress = ProgressMeter(\n",
    "        total_steps,\n",
    "        [batch_time, data_time, losses, top1, top5]\n",
    "    )\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    best_acc1 = 0\n",
    "\n",
    "    def infinite_loader():\n",
    "        while True:\n",
    "            yield from train_loader\n",
    "\n",
    "    for step, (images, target) in zip(range(start_step + 1, total_steps + 1), infinite_loader()):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # move data to the same device as model\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "        step_loss = step_acc1 = step_acc5 = 0.0\n",
    "\n",
    "        for img, trt in zip(images.chunk(accum_freq), target.chunk(accum_freq)):\n",
    "            # compute output\n",
    "            output = model(img)\n",
    "            loss = criterion(output, trt)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, trt, topk=(1, 5), class_prob=True)\n",
    "            step_loss += loss.item()\n",
    "            step_acc1 += acc1[0].item()\n",
    "            step_acc5 += acc5[0].item()\n",
    "            \n",
    "            # compute gradient\n",
    "            (loss / accum_freq).backward()\n",
    "\n",
    "        step_loss /= accum_freq\n",
    "        step_acc1 /= accum_freq\n",
    "        step_acc5 /= accum_freq\n",
    "\n",
    "        losses.update(step_loss, images.size(0))\n",
    "        top1.update(step_acc1, images.size(0))\n",
    "        top5.update(step_acc5, images.size(0))\n",
    "\n",
    "        # do SGD step\n",
    "        l2_grads = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if step % print_freq == 0:\n",
    "            print(step)\n",
    "            progress.display(step)\n",
    "            if wandb:\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    l2_params = sum(p.square().sum().item() for _, p in model.named_parameters())\n",
    "\n",
    "                samples_per_second_per_gpu = batch_size / batch_time.val\n",
    "                samples_per_second = samples_per_second_per_gpu \n",
    "                log_data = {\n",
    "                    \"train/loss\": step_loss,\n",
    "                    'train/acc@1': step_acc1,\n",
    "                    'train/acc@5': step_acc5,\n",
    "                    \"data_time\": data_time.val,\n",
    "                    \"batch_time\": batch_time.val,\n",
    "                    \"samples_per_second\": samples_per_second,\n",
    "                    \"samples_per_second_per_gpu\": samples_per_second_per_gpu,\n",
    "                    \"lr\": scheduler.get_last_lr()[0],\n",
    "                    \"l2_grads\": l2_grads.item(),\n",
    "                    \"l2_params\": math.sqrt(l2_params)\n",
    "                }\n",
    "                wandb.log(log_data, step=step)\n",
    "\n",
    "        if step % log_steps == 0 or step == total_steps:\n",
    "\n",
    "            acc1 = validate(val_loader, original_model, criterion, step)\n",
    "\n",
    "            # remember best acc@1 and save checkpoint\n",
    "            is_best = acc1 > best_acc1\n",
    "            best_acc1 = max(acc1, best_acc1)\n",
    "            \n",
    "            save_checkpoint({\n",
    "                'step': step,\n",
    "                'state_dict': original_model.state_dict(),\n",
    "                'best_acc1': best_acc1,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "                'scheduler' : scheduler.state_dict()\n",
    "            }, is_best,checkpoint_path)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "train(train_loader, val_loader, start_step, total_steps, original_model, model, criterion, optimizer, scheduler, device)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d569b",
   "metadata": {
    "papermill": {
     "duration": 0.020614,
     "end_time": "2024-11-22T23:32:28.604008",
     "exception": false,
     "start_time": "2024-11-22T23:32:28.583394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1500837,
     "sourceId": 2491748,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7311.445237,
   "end_time": "2024-11-22T23:32:31.342622",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-22T21:30:39.897385",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
